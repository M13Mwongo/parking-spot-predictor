{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding:2rem;font-size:100%;text-align:left;display:fill;border-radius:0.25rem;overflow:hidden;background-image: url(https://images.pexels.com/photos/2860804/pexels-photo-2860804.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)\"><b><span style='color:white'> PARKING ANALYSIS PREDICTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/carpark_image.jpg' alt='Carpark Image'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collaborative group project done at the end of Phase 4 of Moringa School's Data Science program. The team members of this group include:\n",
    "1. [Ezra Kipchirchir](https://github.com/dev-ezzy)\n",
    "2. [Grace Mutuku](https://github.com/GraceKoki)\n",
    "3. [Joy Ogutu](https://github.com/Ogutu01)\n",
    "4. [Mary Gaceri](https://github.com/MaryGaceri)\n",
    "5. [Mwiti Mwongo](https://github.com/M13Mwongo)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic is a nightmare, am I right? You canâ€™t drive anywhere without being stuck in traffic for a while, especially in Nairobi. What makes it worse is that a lot of times during high-traffic periods, such as the mornings and evenings, there is a high likelihood of missing out on your desired parking spot that is near your office, especially when looking at county-run parking.\n",
    "This application hopes to predict the parking patterns and likelihood of having available parking spots in certain areas at a given time of the day.\n",
    "\n",
    "Using data analytics and machine learning techniques, we explore the field of **parking prediction** and **urban mobility**. Our research develops a state-of-the-art algorithm that can effectively estimate parking spot availability in metropolitan locations by utilizing real-time parking occupancy data from several sources inclusive of historical records. We welcome you to journey with us as we explore the inner workings of our prediction model, emphasize significant discoveries, and demonstrate how it may revolutionize urban parking system optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Table of contents__\n",
    "\n",
    "- [Business Understanding](#PROJECT-OVERVIEW)\n",
    "- [Data Sourcing](#DATA-SOURCING)\n",
    "- [Data Understanding](#DATA-UNDERSTANDING)\n",
    "- [Data Preprocessing](#DATA-PREPROCESSING)\n",
    "- [Exploratory Data Analysis](#EXPLORATORY-DATA-ANALYSIS)\n",
    "- [Time Series Modelling](#TIME-SERIES-MODELLING)\n",
    "- [Conclusion and Recommendation](#Conclusion-and-Recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Business Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a parking place in the busy urban environments of major cities throughout the world is a problem that worries locals, commuters, and tourists equally. The need for effective parking solutions is greater than ever due to the fast urbanization, rising traffic, and expanding population. Our initiative is to change parking management in metropolitan areas confronting comparable difficulties throughout the globe by utilizing data-driven insights in response to this urgent issue. It is impossible to exaggerate the significance of this endeavor for metropolitan areas. Urban centers are the epicenters of activity, drawing millions of people for business, pleasure, and employment because they are social, cultural, and economic magnets. Nonetheless, these cities' disorganized traffic and inadequate parking facilities provide serious difficulties for local government, companies, and citizens. Our initiative intends to improve urban mobility by reducing travel times, relieving traffic congestion, and offering accurate estimates of parking spot availability.\n",
    "\n",
    "One of the key challenges lies in accessing reliable data on parking occupancy and usage patterns. Parking spaces in urban areas are often managed by various entities, including public agencies, private operators, and informal attendants, making data collection a complex and fragmented process. Moreover, concerns about data privacy and security have hindered efforts to gather comprehensive parking data, as authorities are cautious about disclosing sensitive information due to security reasons.\n",
    "\n",
    "Despite these challenges, our project aims to collaborate with relevant stakeholders, including:\n",
    "\n",
    "- **City authorities**: Vital for regulatory support, infrastructure planning, and policy implementation to enhance urban mobility and parking management.\n",
    "- **Parking operators**: Key players responsible for managing parking facilities, providing valuable data, and implementing innovative solutions to optimize parking spot utilization.\n",
    "- **Technology partners**: Essential for developing and implementing data-driven tools, such as predictive models and smart parking systems, to improve parking availability and streamline operations.\n",
    "- **Motorists (end-users)**: The primary beneficiaries of improved parking management solutions, as they will benefit from reduced search time, enhanced convenience, and better access to parking spots.\n",
    "\n",
    "By fostering partnerships and promoting transparency, we seek to establish a data-sharing framework that respects privacy concerns while enabling the development of innovative solutions to improve parking management in urban centers worldwide. Optimizing parking and reducing congestion, enhances business efficiency, attracts investments, and stimulates economic activity. Encouraging alternative transportation modes reduces emissions and contributes to environmental conservation. Through data analytics, stakeholder collaboration, and innovative tech, we aim to create smarter, more efficient urban mobility ecosystems benefiting all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absence of accurate and up-to-date data on parking spot availability not only impedes the development of effective predictive models but also limits the implementation of innovative solutions aimed at addressing urban mobility challenges. Without access to comprehensive data sources, parking prediction systems struggle to provide reliable real-time information, leading to suboptimal parking decisions and increased traffic congestion. Overcoming these challenges is crucial for creating a parking prediction system that not only improves parking navigation but also contributes to the overall sustainability and livability of urban areas by enhancing economic productivity, and fostering a more seamless urban mobility experience for all stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **MAIN OBJECTIVE**\n",
    "Develop a robust time series-based parking spot predictor that accurately forecasts parking spot availability in urban areas, leveraging historical parking data and real-time variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **OTHER OBJECTIVES**\n",
    "1. To collect and preprocess historical parking data from various sources and integrate relevant time-varying features, such as time of day, day of the week and holidays into the predictive model.\n",
    "\n",
    "2. To explore various time series forecasting techniques, including ARIMA and SARIMA and evaluate the performance of each technique using metrics like accuracy, precision, recall, and F1-score. \n",
    "\n",
    "3. To develop and deploy a user-friendly interface or mobile application that allows motorists to access real-time parking predictions and navigate to available parking spots efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Important Background Information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **a) Imports & OOP**\n",
    "\n",
    "The necessary libraries were first imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "# Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "import itertools\n",
    "import requests\n",
    "import json\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "from requests import api\n",
    "import time\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.pylab import rcParams\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Modeling libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import train_test_split        \n",
    "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline \n",
    "from pmdarima import auto_arima      \n",
    "from prophet import Prophet \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Model deployment libraries\n",
    "import pickle \n",
    "# import streamlit \n",
    "\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom Options for displaying rows.\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In object-oriented programming (OOP), classes serve as blueprints, dividing code into modular components that contain data and actions. They encourage encapsulation, abstraction, and inheritance in code, making it more modular, readable, and manageable. Classes offer an organized way to developing and implementing code, promoting clarity and efficiency in software development.\n",
    "\n",
    "Consequently, the following classes were implemented and defined below:\n",
    "1. Data Sourcing\n",
    "2. Data Understanding\n",
    "3. Data Preprocessing\n",
    "4. Data Analysis\n",
    "5. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSourcing:\n",
    "  def __init__(self,df_carparks,df_carpark_structure,df_carpark_history,df_carparks_zones_merged,df_carparks_zones_coords_merged,df_holidays):\n",
    "    \n",
    "    self.carparks_all = df_carparks\n",
    "    self.carpark_structure = df_carpark_structure\n",
    "    self.carpark_history = df_carpark_history\n",
    "    self.carparks_zones = df_carparks_zones_merged\n",
    "    self.carparks_zones_coords = df_carparks_zones_coords_merged\n",
    "    self.holidays = df_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUnderstanding(DataSourcing):\n",
    "  def __init__(self, data_sourcing_object):\n",
    "    if (isinstance(data_sourcing_object, DataSourcing)):\n",
    "      self.carparks_all = data_sourcing_object.carparks_all\n",
    "      self.carpark_structure = data_sourcing_object.carpark_structure\n",
    "      self.carpark_history = data_sourcing_object.carpark_history\n",
    "      self.holidays = data_sourcing_object.holidays\n",
    "    else:\n",
    "      raise TypeError('data_sourcing_object must be an instance of DataSourcing')\n",
    "\n",
    "  def carpark_names(self):\n",
    "    return self.carparks_all\n",
    "  \n",
    "  def carpark_details(self):\n",
    "    message = f\"\"\"\n",
    "    There are {self.carparks_all.shape[0]} carparks in the dataset.\n",
    "    \n",
    "    The highest number of parking spots available is {self.carpark_structure['spots'].max()}, found at {self.carpark_structure.loc[self.carpark_structure['spots'].idxmax(), 'facility_name']}.\n",
    "    \n",
    "    The lowest number of parking spots available is {self.carpark_structure['spots'].min()}, found at {self.carpark_structure.loc[self.carpark_structure['spots'].idxmin(), 'facility_name']}.\n",
    "    \n",
    "    There are {len(self.carpark_structure.columns)} columns in the dataset: namely {self.carpark_structure.columns.to_list()}\n",
    "    \"\"\"\n",
    "    print(message)\n",
    "    return None\n",
    "\n",
    "  def examine_carpark_history(self):\n",
    "    print(\" ################### Details about the data ################### \\n \")\n",
    "    print(f\"The dataset is a DataFrame with {self.carpark_history.shape[0]} rows and {self.carpark_history.shape[1]} columns\\n\")\n",
    "    print(\"Columns of the dataset:\", self.carpark_history.columns.to_list())\n",
    "    print(\"\\nFirst 5 records of the dataset \")\n",
    "    display(self.carpark_history.head())\n",
    "    \n",
    "    # Display information about the dataset\n",
    "    print(\"\\nData information\")\n",
    "    display(self.carpark_history.info())\n",
    "    print(\"\\nNull Values \")\n",
    "    display(self.carpark_history.isnull().sum())\n",
    "    # print(\"\\nDuplicate Values \")\n",
    "    # print(self.carpark_history.duplicated(), 'duplicate values')\n",
    "    display(self.carpark_history.describe())\n",
    "  \n",
    "    print('\\nData Details')\n",
    "    print(f'Number of unique Parking Facilities:', self.carpark_history.facility_name.nunique())\n",
    "    # print(f'Number of unique days:', self.carpark_history.date.nunique())\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing(DataUnderstanding):\n",
    "  def __init__(self, data_understanding_object):\n",
    "    if (isinstance(data_understanding_object, DataUnderstanding)):\n",
    "      self.carparks_all = data_understanding_object.carparks_all\n",
    "      self.carpark_structure = data_understanding_object.carpark_structure\n",
    "      self.carpark_history = pd.read_parquet('./data/carpark_history_6_months_zones_coords.parquet')\n",
    "      self.holidays = data_understanding_object.holidays\n",
    "    else:\n",
    "      raise TypeError('data_understanding_object must be an instance of DataUnderstanding')\n",
    "\n",
    "  def drop_duplicate_carpark_history(self):\n",
    "    self.carpark_history = self.carpark_history.drop_duplicates()\n",
    "    return self.carpark_history.head()\n",
    "\n",
    "  def drop_facility_ids(self):\n",
    "    # Drop records where facility_id is between 486 and 490\n",
    "    self.carpark_structure = self.carpark_structure[~ self.carpark_structure['facility_id'].between(486, 490)]\n",
    "\n",
    "    # Drop records where facility_id is between 1 and 5\n",
    "    self.carpark_structure = self.carpark_structure[~ self.carpark_structure['facility_id'].between(1, 5)]\n",
    "    \n",
    "    return self.carpark_structure.head()\n",
    "  \n",
    "  def merge_zones_and_carpark_history(self):\n",
    "    # TODO - TEST THIS FUNCTION TO MAKE SURE IT WORKS\n",
    "    # Creating merged dataframe\n",
    "    df = pd.merge(self.carpark_history, self.carpark_history_zones_only, how='outer',left_index=True, right_index=True)\n",
    "    \n",
    "    # Dropping the zones column now that the data is merged\n",
    "    df.drop(columns=['zones'],inplace=True)\n",
    "    \n",
    "    # Renaming the spots column to total_parking_spots\n",
    "    df.rename(columns={'spots':'total_parking_spots'},inplace=True)\n",
    "    \n",
    "    # Ensuring the occupancy_total and total_parking_spots are integers\n",
    "    df['occupancy_total'] = df['occupancy_total'].astype(int)\n",
    "    df['total_parking_spots'] = df['total_parking_spots'].astype(int)\n",
    "    \n",
    "    # Assigning the df to self.carpark_history\n",
    "    self.carpark_history = df\n",
    "\n",
    "    return self.carpark_history.head()\n",
    "\n",
    "  def merge_coords_and_carpark_history(self):\n",
    "    df_facility_coords = pd.read_json('./data/coords.json')\n",
    "\n",
    "    # Creating merged dataframe\n",
    "    merged_df = self.carpark_history.merge(df_facility_coords, on='facility_id',how='outer')\n",
    "    \n",
    "    print(merged_df.head())\n",
    "    # Update 'longitude' and 'latitude' columns where the condition is met\n",
    "    # merged_df['longitude'].combine\n",
    "    \n",
    "    df_facility_coords['longitude'] = merged_df['longitude'].combine_first(df_facility_coords['longitude'])\n",
    "    df_facility_coords['latitude'] = merged_df['latitude'].combine_first(df_facility_coords['latitude'])\n",
    "    \n",
    "    self.carpark_history = merged_df\n",
    "    \n",
    "    return self.carpark_history.head()\n",
    "  \n",
    "  def save_dataframe_to_parquet(self,dataframe,path):\n",
    "    dataframe.to_parquet(path)\n",
    "    print(\"File saved!\")\n",
    "    return None\n",
    "\n",
    "  def extract_date_time_dayOfWeek(self):\n",
    "    # Extracting date and time from MessageDate\n",
    "    self.carpark_history[['date','time']] = self.carpark_history['MessageDate'].str.split('T',expand=True)\n",
    "    \n",
    "    # Combining date and time into a single datetime column\n",
    "    self.carpark_history['datetime'] = pd.to_datetime(self.carpark_history['date'] + ' ' + self.carpark_history['time'])\n",
    "    \n",
    "    # Using the newly created fields to establish the day of the week\n",
    "    self.carpark_history['day_of_week'] = pd.to_datetime(self.carpark_history['date']).dt.day_name()\n",
    "    \n",
    "    # Converting time to timedelta object\n",
    "    self.carpark_history['time'] = pd.to_datetime(self.carpark_history['time']).dt.time\n",
    "    \n",
    "    # Extracting hour from datetime\n",
    "    self.carpark_history['hour'] = self.carpark_history['datetime'].dt.hour\n",
    "    \n",
    "    # Creating time_category column\n",
    "    self.carpark_history['time_category'] = self.carpark_history['hour'].apply(lambda x: self.categorize_time(x))\n",
    "    \n",
    "    # Convert date to datetime object\n",
    "    self.carpark_history['date'] = pd.to_datetime(self.carpark_history['date'])\n",
    "    \n",
    "    # Extract month name from date and assign it to a new month column\n",
    "    self.carpark_history['month'] = self.carpark_history['date'].dt.strftime('%B')\n",
    "    \n",
    "    # # Convert time to time object\n",
    "    # self.carpark_history['time'] = self.carpark_history['time'].dt.time\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    self.carpark_history.drop(['MessageDate','datetime','hour'], axis=1, inplace=True) \n",
    "    \n",
    "    return self.carpark_history.head()\n",
    "\n",
    "  def categorize_carpark_time(self):\n",
    "    self.carpark_history['time_category'] = self.carpark_history['time'].apply(lambda x: self.categorize_time(x.hour))\n",
    "    \n",
    "    return self.carpark_history.head()\n",
    "\n",
    "  def keep_certain_cols(self,columns_to_keep):\n",
    "    # Drop columns in zones\n",
    "    self.carpark_history = self.carpark_history[columns_to_keep]\n",
    "    return self.carpark_history.head()\n",
    "\n",
    "  def rename_cols_in_carpark_history(self):\n",
    "    self.carpark_history.rename(columns={'spots_x': 'capacity'},inplace=True)\n",
    "    self.carpark_history.rename(columns={'occupancy.total': 'occupancy'},inplace=True)\n",
    "    return self.carpark_history.head()\n",
    "  \n",
    "  def create_parking_availability(self):\n",
    "    self.carpark_history['parking_availability'] = self.carpark_history['capacity'] - self.carpark_history['occupancy']\n",
    "    return self.carpark_history.head()\n",
    "  \n",
    "  def holiday_formatting(self):\n",
    "    # Convert 'date' column to datetime format\n",
    "    self.holidays['date'] = pd.to_datetime(self.holidays['date'], format='%b %d')\n",
    "\n",
    "    # Extract month and day information and format it as 'Month Day' in self.holidays\n",
    "    self.holidays['month_day'] = self.holidays['date'].dt.strftime('%m-%d')\n",
    "\n",
    "    # Create a set of holiday month-day combinations\n",
    "    holidays_month_day = set(self.holidays['month_day'])\n",
    "\n",
    "    # Check if the month-day combination of each date in df1 matches any holiday month-day combination\n",
    "    self.carpark_history['month_day'] = self.carpark_history['date'].dt.strftime('%m-%d')\n",
    "    self.carpark_history['is_holiday'] = self.carpark_history['month_day'].isin(holidays_month_day)\n",
    "\n",
    "    # Map True/False to 'yes'/'no' for 'is_holiday' column\n",
    "    self.carpark_history['is_holiday'] = self.carpark_history['is_holiday'].map({True: 'Yes', False: 'No'})\n",
    "\n",
    "    # Drop the temporary 'month_day' column\n",
    "    self.carpark_history.drop(columns=['month_day'], inplace=True)\n",
    "\n",
    "    # Display the DataFrame to verify changes\n",
    "    return self.carpark_history.head()\n",
    "  \n",
    "  \n",
    "  # Helper functions that do not directly modify content in the object instance\n",
    "  @staticmethod\n",
    "  def categorize_time(hour):\n",
    "    if 4 <= hour < 7:\n",
    "        return 'Early Morning'\n",
    "    elif 7 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 15:\n",
    "        return 'Afternoon'\n",
    "    elif 15 <= hour < 18:\n",
    "        return 'Late Afternoon'\n",
    "    elif 18 <= hour < 20:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARN - Potential issue with how the data analysis constructor has been made. Test it out and find out\n",
    "class DataAnalysis(DataPreprocessing):\n",
    "  def __init__(self, data_preprocessing_object):\n",
    "    if (isinstance(data_preprocessing_object, DataPreprocessing)):\n",
    "      self.carparks_all = data_preprocessing_object.carparks_all\n",
    "      self.carpark_structure = data_preprocessing_object.carpark_structure\n",
    "      self.carpark_history = data_preprocessing_object.carpark_history\n",
    "      self.holidays = data_preprocessing_object.holidays\n",
    "    else:\n",
    "      raise TypeError('data_preprocessing_object must be an instance of DataPreprocessing')\n",
    "\n",
    "  def preprocess_dataframe(self):\n",
    "    # Group by 'date' and 'facility_name', and calculate the mean for 'parking_availability' and 'occupancy'\n",
    "    processed_df = self.carpark_history.groupby(['date', 'facility_name']).agg({'parking_availability': 'mean', 'occupancy': 'mean', 'capacity': 'mean', 'longitude' : 'mean', 'latitude' : 'mean'}).reset_index()\n",
    "\n",
    "    return processed_df\n",
    "  \n",
    "  def create_distribution_plots(self):\n",
    "    # List of columns to include in distribution plots\n",
    "    columns = ['capacity', 'parking_availability', 'occupancy', 'longitude', 'latitude']\n",
    "\n",
    "    # Preprocess the dataframe to calculate means for 'parking_availability' and 'occupancy'\n",
    "    processed_df = self.preprocess_dataframe(self.carpark_history)\n",
    "\n",
    "    # Loop through each column in the DataFrame\n",
    "    for column in columns:\n",
    "        # Check if the column is numerical\n",
    "        if pd.api.types.is_numeric_dtype(processed_df[column]):\n",
    "            # Create a figure with subplots\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "\n",
    "            # Histogram with KDE\n",
    "            sns.histplot(processed_df[column], kde=True, ax=axes[0])\n",
    "            axes[0].set_title(f'Distribution Plot for {column}')\n",
    "            axes[0].set_xlabel(column)\n",
    "            axes[0].set_ylabel('Frequency')\n",
    "\n",
    "            # Boxplot\n",
    "            sns.boxplot(x=processed_df[column], ax=axes[1])\n",
    "            axes[1].set_title(f'Boxplot for {column}')\n",
    "            axes[1].set_xlabel(column)\n",
    "            axes[1].set_ylabel('')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelling(DataAnalysis):\n",
    "  def __init__(self, data_analysis_object):\n",
    "    if (isinstance(data_analysis_object, DataAnalysis)):\n",
    "      self.carparks_all = data_analysis_object.carparks_all\n",
    "      self.carpark_structure = data_analysis_object.carpark_structure\n",
    "      self.carpark_history = data_analysis_object.carpark_history\n",
    "      self.holidays = data_analysis_object.holidays\n",
    "    else:\n",
    "      raise TypeError('data_understanding_object must be an instance of DataUnderstanding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The API - whose base URL was `https://api.transport.nsw.gov.au/v1/carpark` - had two endpoints:\n",
    "1. `{baseURL}?facility={facility_id}` - Containts one optional variable ***facility_id***. Returns occupancy details of a car park based on a facility ID. If the facility ID specified, a list of facility names with their ID will be returned.\n",
    "2. `{baseURL}}/history?facility={facility_id}&eventdate={date_in_question}` - Contains two mandatory variables, ***facility_id*** and ***date_in_question*** formatted as *YYYY-MM-DD*. Returns historical occupancy details of a car park based on a facility ID\n",
    "and event date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **b) The Process of Fetching Data**\n",
    "\n",
    "Our data was sourced from the Transport for New South Wales(TfNSW) website, more speficially, from their [Car Park API](https://opendata.transport.nsw.gov.au/dataset/car-park-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our intention was to use this API to fetch six months' worth of historical parking data. An extensive time period would lead to a proper understanding of parking habits across a wide array of conditions while factoring in social events, public holidays, school holidays and even leave days of employees.\n",
    "\n",
    "The team came up with code to automatically make requests to the API, and save this information in a dataframe. However, after further study of the API's structure and the data being received, the team saw it best to have these requests made once and the resulting data stored in json files, which can be read by pandas.\n",
    "\n",
    "The function below was used to retrieve car park data from the TfNSW API and saves it to a JSON file. It will then read the JSON file into a dataframe, rename the columns as they come with no name from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_carparks_list():\n",
    "  dotenv.load_dotenv('.env')\n",
    "  # path to json file created/saved\n",
    "  carparks_file_path = './data/carparks.json'\n",
    "  # Delete any existing file at carparks path\n",
    "  os.remove(carparks_file_path) if os.path.exists(carparks_file_path) else None\n",
    "\n",
    "  # Creating header for request\n",
    "  headers = {\n",
    "      \"Authorization\": f\"apikey {os.environ.get('apikey')}\"\n",
    "  }\n",
    "  # Specifying url to get carparks\n",
    "  url_carparks = 'https://api.transport.nsw.gov.au/v1/carpark'\n",
    "\n",
    "  list_of_carparks = requests.get(url_carparks, headers=headers).json()\n",
    "\n",
    "  df_carparks = pd.DataFrame.from_dict(list_of_carparks, orient='index')\n",
    "  # Resetting the index to label the columns afterwards\n",
    "  df_carparks = df_carparks.reset_index()\n",
    "  df_carparks.columns = ['facility_id', 'CarParkName']\n",
    "\n",
    "  # Deleting old file\n",
    "  os.remove(carparks_file_path) if os.path.exists(carparks_file_path) else None\n",
    "\n",
    "  # Creating new file with updated column titles\n",
    "  pd.DataFrame.to_json(df_carparks, carparks_file_path)\n",
    "\n",
    "  print('File created and updated successfully.')\n",
    "  return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the names of the various facilites, the structure of each of the carparks was investigated. It was noted that each car park can have a different configuration, where each facility may have one or more car parks, and each car park may have one or more zones as depicted below.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src='./images/carpark_structure.png' alt='Carpark structure'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing this, the function below was created to fetch the individual details of the carparks - using the JSON file just created - to properly scrutinise their structure. This would then be saved in its own JSON file named `carpark_structure.json` for future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_carpark_structure(path_to_carpark_json_file):\n",
    "  # Delete file found at same path\n",
    "  os.remove('./data/carpark_structure.json') if os.path.exists('./data/carpark_structure.json') else None\n",
    "\n",
    "  # Add file to dataframe\n",
    "  df_carparks = pd.read_json(path_to_carpark_json_file)\n",
    "  # Initialise array that will hold information\n",
    "  carpark_details_array = []\n",
    "\n",
    "  # Loop through carparks to get information\n",
    "  for index, row in df_carparks.iterrows():\n",
    "    facility = row['facility_id']\n",
    "    url = f'https://api.transport.nsw.gov.au/v1/carpark?facility={facility}'\n",
    "\n",
    "    # Creating header for request\n",
    "    headers = {\n",
    "        \"Authorization\": f\"apikey {os.environ.get('apikey')}\"\n",
    "    }\n",
    "    # Make request\n",
    "    response = requests.get(url, headers=headers).json()\n",
    "\n",
    "    # Add to array\n",
    "    carpark_details_array.append(response)\n",
    "\n",
    "  # Store information in JSON file\n",
    "  with open('./data/carpark_structure.json', 'w') as f:\n",
    "    json.dump(carpark_details_array, f)\n",
    "  # Create dataframe and return it\n",
    "  return pd.DataFrame(carpark_details_array)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done that, a new function - named `date_getter` - was created to give a list of all the days in a given time period. This function generates a list of dates based in the input time delta based, taking a time delta as an argument and returns a list of dates in the format \"YYY-MM-DD\".\n",
    "\n",
    "This would be useful as carpark history for each of the carparks within a given time delta would be needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def date_getter(td):\n",
    "    # Array that stores the dates to be searched for\n",
    "    date_period_list = []\n",
    "\n",
    "    # The last date to be searched for\n",
    "    cutoff_date = datetime(2023, 12, 31)\n",
    "    target_date = cutoff_date - td\n",
    "\n",
    "    # Ensure that records of each day are obtained\n",
    "    delta = timedelta(days=1)\n",
    "\n",
    "    while target_date <= cutoff_date:\n",
    "        date_period_list.append(target_date.strftime(\"%Y-%m-%d\"))\n",
    "        target_date += delta\n",
    "\n",
    "    return date_period_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a date function, a new function (`get_carpark_history`) was made to fetch the carpark history of a particular facility across a range of dates.\n",
    "\n",
    "This function is used to get carpark history data for a specific facility and dates, taking the name of the carpark facility and the list of dates for which to retrieve carpark history data as arguments. It returns a dataFrame containing the carpark history data, while saving the data into a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_carpark_history(facility, dates_array):\n",
    "\n",
    "    # Initialize data array\n",
    "    data_array = []\n",
    "\n",
    "    # Define the path for the JSON file\n",
    "    json_file_path = f\"./data/carpark history/facility_{facility}.json\"\n",
    "\n",
    "    # Set the request header\n",
    "    headers = {\n",
    "        \"Authorization\": f\"apikey {os.environ.get('apikey')}\"\n",
    "    }\n",
    "\n",
    "    # Delete the file if it exists\n",
    "    if os.path.exists(json_file_path):\n",
    "        os.remove(json_file_path)\n",
    "\n",
    "    # Make a request for each date and aggregate the data\n",
    "    for date in dates_array:\n",
    "        url = f'https://api.transport.nsw.gov.au/v1/carpark/history?facility={facility}&eventdate={date}'\n",
    "        response = requests.get(url, headers=headers).json()\n",
    "\n",
    "        if data_array == []:\n",
    "            data_array = response\n",
    "        else:\n",
    "            data_array = data_array + response\n",
    "\n",
    "    # Save the data to a JSON file\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump(data_array, f)\n",
    "\n",
    "    # Read the JSON file\n",
    "    with open(json_file_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert the read data into a pandas DataFrame\n",
    "    return pd.DataFrame(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data Sourcing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data was sourced from the Transport for New South Wales(TfNSW) website, more speficially, from their [Car Park API](https://opendata.transport.nsw.gov.au/dataset/car-park-api).\n",
    "\n",
    "The API - whose base URL was `https://api.transport.nsw.gov.au/v1/carpark` - had two endpoints:\n",
    "1. `{baseURL}?facility={facility_id}` - Containts one optional variable ***facility_id***. Returns occupancy details of a car park based on a facility ID. If the facility ID specified, a list of facility names with their ID will be returned.\n",
    "2. `{baseURL}}/history?facility={facility_id}&eventdate={date_in_question}` - Contains two mandatory variables, ***facility_id*** and ***date_in_question*** formatted as *YYYY-MM-DD*. Returns historical occupancy details of a car park based on a facility ID\n",
    "and event date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was sourced over a 6 month period, from the beginning of July 2023 to 31st December 2023. A loop was created for each facility using the given date range, and the `get_carpark_history` function was run within that loop. The respective files that were saved contained the parking history of that facility for the 6-month time period (found in *./data/carpark_history_6_months/facility_<<facility_id>>*). However, in a bid to simplify the starting point and to ensure that one dataframe is used as our starting point, the code below was implemented to read all the data from the various parquet files and put it in one file, from which the one dataframe was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir('data/carpark_history_6_months'):\n",
    "  df_file = pd.read_parquet('data/carpark_history_6_months/' + file)\n",
    "    \n",
    "    if file == 'facility_6.parquet':\n",
    "      df = df_file\n",
    "    else:\n",
    "      df = pd.concat([df,df_file]).reset_index(drop=True)\n",
    "\n",
    "# Save to parquet\n",
    "df.to_parquet('data/carpark_history_6_months.parquet')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parquet file was chosen due as its columnar storage format is highly efficient for both reading and writing large datasets due to its compression and columnar layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carpark_history = pd.read_parquet('./data/carpark_history_6_months.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Moving on, the data containing the parking lot structure as well as the parking lot names can now be converted to a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_carparks = pd.read_json('./data/carparks.json')\n",
    "df_carpark_structure = pd.read_json('./data/carpark_structure.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the files containing the geolocation coordinates of each parking facility, as well as the public holiday information for Australia were loaded into their own respective dataframes. This information will come in handy later on. Files with the carparks and zones merged, as well as the carparks, zones and coordinates merged, were loaded into their own dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the holiday data\n",
    "df_holidays = pd.read_csv('./data/NSW_holidays_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataframe with carparks and zones merged\n",
    "df_carparks_zones_merged = pd.read_parquet('./data/carpark_history_6_months_with_zones.parquet')\n",
    "\n",
    "# Dataframe for the carparks, zones and coordinates merged\n",
    "df_carparks_zones_coords_merged = pd.read_parquet('./data/carpark_history_6_months_zones_coords.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this, all the dataframes can now be passed onto the DataSourcing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sourcing = DataSourcing(df_carparks,df_carpark_structure,df_carpark_history,df_carparks_zones_merged,df_carparks_zones_coords_merged,df_holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Data Understanding:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identification, gathering, and cursory analysis of the data in this part will be carried out by:\n",
    "\n",
    "- Gathering preliminary data, which has been put into a JSON file.\n",
    "- Describing the data that we have at our disposal.\n",
    "- Looking for patterns and correlations in the data.\n",
    "- Confirming the accuracy of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the data understanding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_understanding = DataUnderstanding(data_sourcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this, a general summary of the all the carparks' parking history is outlined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_understanding.examine_carpark_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further look at the names and facility_ids of the various carparks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_understanding.carparks_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closer look at the detailed structure of the carparks was also done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_understanding.carpark_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the carparks was shown as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_understanding.carpark_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Data Preprocessing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new instance of the DataPreprocessing object is first instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing = DataPreprocessing(data_understanding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, various steps in the data preprocessing lifecycle can be carried out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dealing with Outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers were kept in the dataset for a variety of reasons. Preserving outliers ensures the integrity of the data by accurately reflecting the underlying distribution and real-world phenomena. This decision helps to prevent valuable information from being discarded, which could potentially skew subsequent analyses or models. \n",
    "\n",
    "Furthermore, retaining outliers guards against bias being introduced into the analysis, as extreme values may represent genuine data points rather than errors. Additionally, the presence of outliers contributes to the robustness of analysis, as it ensures that statistical methods and machine learning algorithms are not unduly influenced by extreme values. Moreover, outliers can provide insights into unique patterns or anomalies within the data, prompting further investigation. \n",
    "\n",
    "Lastly, analyzing outliers aids in identifying data quality issues such as measurement errors or data entry mistakes, facilitating improvements in data collection processes and overall data quality. Thus, while the decision to keep or remove outliers depends on the specific context and objectives of the analysis, retaining outliers is often essential for ensuring accurate and comprehensive data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dropping unneccesary features/duplicates & Renaming Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.carpark_history.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by dropping the columns that aren't needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.keep_certain_cols(columns_to_keep=['facility_id','facility_name','spots_x', 'occupancy.total','MessageDate','longitude','latitude' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then proceed to rename certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.rename_cols_in_carpark_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also go ahead and drop duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.drop_duplicate_carpark_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe contains a crucial column known as `zones` - which is recommended by the API to use - contains crucial information for calculating the parking availability. The column, which contains a list of dictionaries, is extracted from the zone and placed in its own dataframe using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def separate_zones_from_carpark_history(self):\n",
    "\n",
    "    # Converting the zones column to its own dataframe\n",
    "    df_zones = pd.DataFrame(columns=['spots', 'zone_id', 'zone_name', 'parent_zone_id', 'occupancy.loop','occupancy.monthlies','occupancy.open_gate','occupancy.total','occupancy.transients'])\n",
    "    rename_format = {\n",
    "        0: 'spots',\n",
    "        1: 'zone_id',\n",
    "        2: 'zone_name',\n",
    "        3: 'parent_zone_id',\n",
    "        4: 'occupancy_loops',\n",
    "        5: 'occupancy_total',\n",
    "        6: 'occupancy_monthlies',\n",
    "        7: 'occupancy_open_gate',\n",
    "        8: 'occupancy_transients'\n",
    "    }\n",
    "\n",
    "    zones_list = []\n",
    "\n",
    "    for index,row in self.carpark_history.iterrows():\n",
    "        # Normalize values in each record in zones column\n",
    "        df_zone = pd.json_normalize(row['zones'])\n",
    "        \n",
    "        zones_list.append(df_zone)\n",
    "\n",
    "    # Concatendating zones list\n",
    "    df_zones = pd.concat(zones_list, ignore_index=True)\n",
    "    \n",
    "    # Keeping necessary columns\n",
    "    self.carpark_history_zones_only = df_zones[['zone_id','occupancy_total']]\n",
    "\n",
    "    return self.carpark_history_zones_only\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed to create a new column `parking_availability` which is the difference between the capacity and the occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.create_parking_availability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns for the date and time were created from the MessageDate column by spliting it across a common letter. A new column that also had the time_category was created as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.extract_date_time_dayOfWeek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infirmation on the holidays was now included in the carpark history dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.holiday_formatting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adding Geolocation Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lat/long coordinates for each of the carparks was obtained from the API's documentation that the data was sourced and saved in the `coords.json` file. \n",
    "\n",
    "It was combined with the carparks history with zones to create a new file: `carpark_history_6_months_zones_coords.parquet`\n",
    "\n",
    "Meanwhile, as this data was added before, it is present in the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.carpark_history.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain facilities were deemed surplus to requirements when it comes to analysis and modelling. These facilities were the ones with facility_ids 1-5 and 486-490 (inclusive). \n",
    "\n",
    "For facility_ids 1-5, these facilities did not have data for the time period in question. As for facility_ids 486-490, these facilities - as per the API we sourced the data from - had inconsistent data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessing.drop_facility_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Explorative Data Analysis & Visualisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, through a combination of visualizations, statistical summaries, and data manipulation techniques, we delve into the dataset's intricacies, examining the distribution of variables, identifying correlations, and detecting anomalies. By thoroughly exploring the data's structure and characteristics, we aim to gain a deeper understanding of its underlying properties, paving the way for informed hypotheses and refined analysis strategies.\n",
    "\n",
    "First we will start by instantiating a DataAnalysis object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis = DataAnalysis(data_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going ahead to assign a variable to the dataframe that we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = data_analysis.carpark_history\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Univariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the numerical columns is visualized below using distribution plots (histogram with KDE) and boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(dataframe):\n",
    "    # Group by 'date' and 'facility_name', and calculate the mean for 'parking_availability' and 'occupancy'\n",
    "    processed_df = dataframe.groupby(['date', 'facility_name']).agg({'parking_availability': 'mean', 'occupancy': 'mean', 'capacity': 'mean', 'longitude' : 'mean', 'latitude' : 'mean'}).reset_index()\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "def create_distribution_plots(dataframe):\n",
    "    # List of columns to include in distribution plots\n",
    "    columns = ['capacity', 'parking_availability', 'occupancy', 'longitude', 'latitude']\n",
    "\n",
    "    # Preprocess the dataframe to calculate means for 'parking_availability' and 'occupancy'\n",
    "    processed_df = preprocess_dataframe(dataframe)\n",
    "\n",
    "    # Loop through each column in the DataFrame\n",
    "    for column in columns:\n",
    "        # Check if the column is numerical\n",
    "        if pd.api.types.is_numeric_dtype(processed_df[column]):\n",
    "            # Create a figure with subplots\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "\n",
    "            # Histogram with KDE\n",
    "            sns.histplot(processed_df[column], kde=True, ax=axes[0])\n",
    "            axes[0].set_title(f'Distribution Plot for {column}')\n",
    "            axes[0].set_xlabel(column)\n",
    "            axes[0].set_ylabel('Frequency')\n",
    "\n",
    "            # Boxplot\n",
    "            sns.boxplot(x=processed_df[column], ax=axes[1])\n",
    "            axes[1].set_title(f'Boxplot for {column}')\n",
    "            axes[1].set_xlabel(column)\n",
    "            axes[1].set_ylabel('')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Call the function to create distribution plots\n",
    "create_distribution_plots(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Capacity**\n",
    "\n",
    "1. **Distribution Plot for Capacity**:\n",
    "   - This graph illustrates the frequency distribution of capacity. On the x-axis, we have capacity values, and on the y-axis, we see the frequency. The vertical bars represent the number of occurrences (frequency) for each capacity value or range. Additionally, a smooth line overlays the bars to show the distribution trend. Notably, most data points cluster around the 0-250 capacity range, indicating higher frequency in that interval.\n",
    "\n",
    "2. **Boxplot for Capacity**:\n",
    "   - The box represents the interquartile range where 50% of the data exists. Lines extending from either end of the box (whiskers) indicate variability beyond this range, and any points beyond these whiskers are considered to be outliers.\n",
    "\n",
    "The majority of parking facilities have a capacity clustered around the 0-250 range, with few outliers indicating larger parking capacities. The boxplot suggests that most facilities have a relatively consistent capacity, with some variability observed.\n",
    "\n",
    "**Parking Availability**\n",
    "\n",
    "\n",
    "1. **Distribution Plot for parking availability**:\n",
    "   - This graph illustrates the frequency distribution of parking availability. On the x-axis, we have different ranges of parking availability (from 0 to 1750), and on the y-axis, we see the frequency of occurrence. The blue bars represent the number of occurrences (frequency) for each parking availability value or range.\n",
    "   - Notably, there's a significant peak at **zero parking availability**, indicating that this value occurs most frequently. As parking availability increases, the frequency decreases.\n",
    "   - A smooth line overlaid on top of the bars indicates the frequency trend more smoothly.\n",
    "\n",
    "2. **Boxplot for parking availability**:\n",
    "   - The interquartile range (IQR) is approximately between **0 and 500** for parking availability.\n",
    "   - Several **outliers** are present beyond the whiskers of the boxplot.\n",
    "\n",
    "The distribution plot indicates a significant peak at zero parking availability, suggesting that many parking facilities frequently reach full capacity. As availability increases, the frequency of occurrence decreases. The boxplot shows that the interquartile range (IQR) for parking availability is relatively small, indicating less variability in this metric compared to capacity.\n",
    "\n",
    "\n",
    "**Occupancy**\n",
    "\n",
    "\n",
    "1. **Distribution Plot for occupancy**:\n",
    "   - This graph illustrates the frequency distribution of occupancy. On the x-axis, we have different levels of occupancy (ranging from 0 to 800), and on the y-axis, we see the frequency of occurrence. The blue bars represent the number of occurrences (frequency) for each level of occupancy.\n",
    "   - Notably, there's a significant peak at **low levels of occupancy**, indicating that these values occur most frequently. As occupancy increases, the frequency decreases.\n",
    "   - A smooth line overlaid on top of the bars indicates the distribution trend more smoothly.\n",
    "\n",
    "2. **Boxplot for occupancy**:\n",
    "   - The interquartile range (IQR) is large at lower occupancies, suggesting variability in the data.\n",
    "   - Several **outliers** are visible at higher occupancies above 500.\n",
    "\n",
    "Similar to parking availability, there's a peak at low levels of occupancy, indicating frequent instances of low usage. As occupancy increases, the frequency decreases. The boxplot highlights variability in occupancy levels, with some facilities experiencing higher occupancy rates and potential outliers.\n",
    "\n",
    "**Longitude**\n",
    "\n",
    "\n",
    "1. **Distribution Plot for longitude**:\n",
    "   - This graph illustrates the frequency distribution of longitude data. On the x-axis, we have different longitude values, ranging from approximately **-34.6** to **-33.4**. The y-axis represents the **frequency**, with values ranging up to **1200**.\n",
    "   - Notably, there's a prominent peak around **-33.8**, indicating that this longitude value occurs most frequently.\n",
    "   - A smooth line overlays the bars, indicating the distribution trend.\n",
    "\n",
    "2. **Boxplot for longitude**:\n",
    "   - The interquartile range (IQR) is represented by the blue box, which spans from approximately **-34.6** to **-33.4**.\n",
    "   - Lines (whiskers) extend from the box, indicating variability beyond the upper and lower quartiles. Some individual points beyond the whiskers may be potential **outliers**.\n",
    "\n",
    "**Latitude**\n",
    "\n",
    "\n",
    "1. **Distribution Plot for Latitude**:\n",
    "    - On the left side, there's a distribution plot showing the frequency of data points at different latitudes.\n",
    "    - The x-axis represents \"latitude\" with values ranging from 150.7 to 151.3.\n",
    "    - The y-axis represents \"Frequency\" with values ranging from 0 to 700.\n",
    "    - Notably, there are peaks in frequency around latitudes 150.8 and 151.2.\n",
    "\n",
    "2. **Boxplot for Latitude**:\n",
    "    - On the right side, there's a boxplot illustrating the spread of the latitude data.\n",
    "    - The x-axis labels are similar to those in the distribution plot.\n",
    "\n",
    "The distribution plots show the frequency of data points at different longitude and latitude values. While there are peaks indicating common longitude and latitude values, the boxplots reveal variability in these geographic coordinates, with potential outliers suggesting locations that deviate from the norm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Count Plot of Non-Holidays and Holidays from July to December**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order of months from July to December\n",
    "month_order = ['July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Convert 'month' column to categorical with specified order\n",
    "final_df['month'] = pd.Categorical(final_df['month'], categories=month_order, ordered=True)\n",
    "\n",
    "# Group by 'Month' and 'is_holiday' columns and count the occurrences\n",
    "holiday_counts = final_df.groupby(['month', 'is_holiday'])['date'].nunique().reset_index(name='Count')\n",
    "\n",
    "# Create a countplot\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x='month', y='Count', hue='is_holiday', data=holiday_counts, palette='Set2')\n",
    "plt.title('Count of Non-Holidays vs Holidays from July to December')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Day Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Holiday Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph titled \"Count of Non-Holidays vs Holidays from July to December\" provides a clear visualization of the distribution of non-holidays and holidays across the months from July to December. Here's a detailed summary based on the graph:\n",
    "\n",
    "- **July to December Distribution**: The graph spans from July to December along the x-axis, representing these six months. Each month is divided into two sections: one for non-holidays (green bars) and the other for holidays (orange bars).\n",
    "\n",
    "- **Non-Holiday Counts**: The green bars, representing non-holidays, consistently reach nearly 30 days for each month. This indicates that there are almost 30 non-holidays in each month, reflecting a regular pattern of workdays or non-holiday periods.\n",
    "\n",
    "- **Holiday Counts**: In contrast, the orange bars, representing holidays, appear at the bottom of each month and are significantly shorter compared to the green bars. This indicates that the number of holidays is relatively lower compared to non-holidays. \n",
    "\n",
    "- **Legend Explanation**: The legend in the top-right corner provides clarity on the color representation:\n",
    "  - The green color corresponds to \"No\" holiday status.\n",
    "  - The orange color corresponds to \"Yes\" holiday status.\n",
    "\n",
    "The distribution of holiday days across the months shows variation, with some months having fewer holidays than others. It highlights the prevalence of non-holiday days compared to holidays and allows for easy comparison across the months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Bivariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Total Capacity of Parking Spots for each Facility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by facility name and sum the total parking spots\n",
    "facility_parking_spots = final_df.groupby('facility_name')['capacity'].first().reset_index()\n",
    "\n",
    "# Sort the DataFrame by total parking spots in descending order\n",
    "facility_parking_spots = facility_parking_spots.sort_values(by='capacity', ascending=False)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='facility_name', y='capacity', data=facility_parking_spots)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Facility Name')\n",
    "plt.ylabel('Total Parking Spots')\n",
    "plt.title('Total Parking Spots by Facility (Descending Order)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph titled \"Total Parking Spots by Facility (Descending Order)\" offers a comprehensive overview of the distribution of parking spots across different facilities. Here's a detailed summary based on the graph:\n",
    "\n",
    "- **Facility Distribution**: The x-axis lists the names of various facilities (car parks), providing a clear identification of each entity included in the analysis and the y-axis represents the total number of parking spots, ranging from 0 to 1750. Each bar on the graph corresponds to a specific facility and depicts the total number of parking spots available at that location.\n",
    "\n",
    "- **Descending Order**: The bars are arranged in descending order, with the facility having the highest number of parking spots positioned at the top of the graph. This arrangement facilitates quick identification of facilities with the most parking spots. **Leppington Car Park** emerges as the facility with the highest number of spots, while **Kiama Car Park** registers the lowest count.\n",
    "\n",
    "This graph effectively presents the distribution of parking spots across various facilities, enabling stakeholders to identify facilities with the highest and lowest parking capacity at a glance. It serves as a valuable tool for decision-making and resource allocation related to parking management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a side-by-side boxplot or violin plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Side-by-side boxplot\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='time_category', y='parking_availability',\n",
    "            data=final_df, palette='Set2')\n",
    "plt.title('Parking Availability Distribution Across Time Intervals')\n",
    "\n",
    "# Side-by-side violin plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.violinplot(x='time_category', y='occupancy',\n",
    "               data=final_df, palette='Set2')\n",
    "plt.title('Occupancy Rate Distribution Across Time Intervals')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Parking Availability Distribution Across Time Intervals**\n",
    "\n",
    "The green bars represent the number of available parking spots during different times of day.\n",
    "    \n",
    "- **Night**: There are ample parking spots available during the night.\n",
    "- **Early Morning**: Availability remains high.\n",
    "- **Morning**: Still a good number of spots.\n",
    "- **Afternoon**: Availability starts to decrease slightly.\n",
    "- **Late Afternoon**: A dip in availability.\n",
    "- **Evening**: The lowest availability, but still some spots.\n",
    "\n",
    "\n",
    "2. **Occupancy Rate Distribution Across Time Intervals**\n",
    "\n",
    " The violin plots show the distribution of occupancy rates.\n",
    "    \n",
    "- **Night**: Occupancy is relatively low, with a wide spread.\n",
    "- **Early Morning**: Occupancy remains low.\n",
    "- **Morning**: A peak around the median occupancy.\n",
    "- **Afternoon**: Occupancy increases, with a narrower spread.\n",
    "- **Late Afternoon**: Highest occupancy, concentrated around the median.\n",
    "- **Evening**: Occupancy decreases slightly.\n",
    "\n",
    "The analysis reveals insights into parking availability and occupancy rates across various time intervals. Parking availability remains relatively consistent throughout the day, with a slight decline observed in the late afternoon and evening. Conversely, occupancy rates peak during the afternoon and late afternoon periods, indicating increased demand for parking spaces during these times. For optimal parking opportunities, it is recommended to target early morning or late afternoon intervals, where a favorable balance between availability and occupancy is likely to be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Monthly Average Occupancy per facility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique facility names\n",
    "facility_names = final_df['facility_name'].unique()\n",
    "\n",
    "# Set the style of the plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "num_facilities = len(facility_names)\n",
    "num_cols = 4  # Number of columns per row\n",
    "num_rows = math.ceil(num_facilities / num_cols)  # Round up to the nearest integer\n",
    "\n",
    "# Create subplots with calculated rows and columns\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(18, num_rows * 5), sharex=True)\n",
    "\n",
    "# Iterate over each facility name and its corresponding axis\n",
    "for idx, (facility_name, ax) in enumerate(zip(facility_names, axes.flatten())):\n",
    "    # Filter data for the current facility\n",
    "    facility_data = final_df[final_df['facility_name'] == facility_name]\n",
    "    \n",
    "    # Group the data by month and calculate the average occupancy total\n",
    "    facility_month_avg = facility_data.groupby('month')['occupancy'].mean().reset_index()\n",
    "    \n",
    "    # Create bar plot for the current facility\n",
    "    sns.barplot(data=facility_month_avg, x='month', y='occupancy', palette='viridis', ax=ax)\n",
    "    \n",
    "    # Set title and labels for the current subplot\n",
    "    ax.set_title(f'Average Occupancy Total for {facility_name}')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Average Occupancy Total')\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(['July', 'August', 'September', 'October', 'November', 'December'], rotation=45)\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide empty subplots if the number of facilities is not a multiple of num_cols\n",
    "if num_facilities % num_cols != 0:\n",
    "    for ax in axes.flatten()[num_facilities:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph illustrates the average occupancy across the months of July through December for a dataset comprising 28 parking facilities. On the x-axis, we have the months, ranging from July to December, while the y-axis represents the average occupancy level. Each data point on the graph corresponds to the average occupancy recorded for a specific month across all 28 parking facilities.\n",
    "\n",
    "The observed trend indicates that December exhibits the lowest average occupancy compared to the preceding months of July through November. This decline in occupancy during December can be attributed to several factors:\n",
    "\n",
    "1. **Holiday Season**: December encompasses the holiday season, characterized by public holidays such as Christmas and New Year's Eve. During this period, many individuals may be on vacation or traveling, resulting in reduced demand for parking facilities and consequently lower occupancy rates.\n",
    "\n",
    "2. **Reduced Workdays**: December often includes public holidays and reduced workdays, leading to fewer commuters and workers requiring parking spaces. This decrease in daily commuting contributes to a decline in overall occupancy levels across parking facilities.\n",
    "\n",
    "3. **Travel Plans**: Some individuals may travel during December for vacations or family gatherings, further reducing the local demand for parking facilities and contributing to lower average occupancy rates.\n",
    "\n",
    "Overall, the combination of holiday-related factors, reduced workdays, school holidays, and altered shopping patterns during December likely contributes to the observed decrease in parking facility occupancy compared to the preceding months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Average Occupancy by Day of the Week**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of days of the week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# First subplot: Average Occupancy by Day of the Week\n",
    "plt.figure(figsize=(10, 5))  # Adjust the figure size as needed\n",
    "sns.barplot(x='day_of_week', y='occupancy', data=final_df, palette='deep', order=day_order)\n",
    "plt.title('Average Occupancy by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Occupancy')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the first plot\n",
    "plt.show()\n",
    "\n",
    "# Second subplot: Occupancy by Day of the Week per Parking Facility\n",
    "plt.figure(figsize=(20, 10))  # Adjust the figure size as needed\n",
    "sns.barplot(x='day_of_week', y='occupancy', data=final_df, palette='viridis', hue='facility_id', order=day_order)\n",
    "plt.title('Occupancy by Day of the Week per Parking Facility')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Occupancy')\n",
    "plt.legend(title='Parking Facility', loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the second plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average Occupancy by Day of the Week**\n",
    "\n",
    "- **Axes**:\n",
    "    - **Y-Axis**: Represents the average occupancy, ranging from **0** to **350**.\n",
    "    - **X-Axis**: Lists the days of the week from **Monday** to **Sunday**.\n",
    "\n",
    "The graph depicts the average occupancy levels across different days, ranging from Monday to Sunday. Tuesday stands out with the highest average occupancy, surpassing 325, while Sunday records the lowest average occupancy, barely surpassing 100. The other days fall between these extremes.\n",
    "\n",
    "\n",
    "\n",
    "**Occupancy by Day of the Week per Parking Facility**\n",
    "\n",
    "- **Axes**:\n",
    "    - **Y-Axis**: Represents the **occupancy level**, ranging from **0** to **900**.\n",
    "    - **X-Axis**: Lists the days of the week from **Monday** to **Sunday**.\n",
    "\n",
    "- **Bars**:\n",
    "    - Each colored bar represents a different **parking facility**, numbered from **6** to **33**.\n",
    "    - The colors of the bars correspond to specific facilities, as indicated in the legend on the right side of the graph.\n",
    "    - Some facilities show **high occupancy**, while others have **lower occupancy** levels.\n",
    "\n",
    "The second graph illustrates occupancy levels across various parking facilities. While some facilities consistently exhibit high occupancy levels, others show lower and more variable occupancy rates across different days of the week.\n",
    "\n",
    "**Recommendations**\n",
    "- **Peak Days**: Consider allocating additional resources (staff, maintenance) on days with high occupancy.\n",
    "- **Facility-Specific Strategies**: Tailor management strategies based on the occupancy patterns of individual facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Average Occupancy Percentage by facility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of occupied slots\n",
    "final_df['OccupancyPercentage'] = (final_df['occupancy'] / final_df['capacity']) * 100\n",
    "\n",
    "# Group by ParkingLotID and calculate average occupancy percentage\n",
    "average_occupancy = final_df.groupby('facility_name')['OccupancyPercentage'].mean().reset_index()\n",
    "\n",
    "# Sort the DataFrame by average occupancy percentage in ascending order\n",
    "average_occupancy = average_occupancy.sort_values(by='OccupancyPercentage')\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(y = average_occupancy['facility_name'], x = average_occupancy['OccupancyPercentage'],\n",
    "            data=final_df, palette='viridis', orient='h')\n",
    "plt.title('Average Parking Lot Occupancy')\n",
    "plt.ylabel('Parking Lot Name')\n",
    "plt.xlabel('Average Occupancy Percentage (%)')\n",
    "# plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar graph above represents the average occupancy percentage of various car parks. Here are the details:\n",
    "\n",
    "**Graph Structure**:\n",
    "- The x-axis represents the \"Average Occupancy Percentage (%)\" ranging from 0% to 70%.\n",
    "- The y-axis lists the names of different car parks. Each bar's length corresponds to the average occupancy percentage of that particular car park.\n",
    "\n",
    "**Car Park Occupancy**:\n",
    "- **Perith Combewood At-Grade Car Park** has the lowest occupancy rate, below 10%.\n",
    "- **Campbelltown Farrow Rd North Car Park** has the highest occupancy rate, close to 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Monthly Average Parking Availability per facility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique facility names\n",
    "facility_names = final_df['facility_name'].unique()\n",
    "\n",
    "# Set the style of the plot\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "num_facilities = len(facility_names)\n",
    "num_cols = 4  # Number of columns per row\n",
    "num_rows = math.ceil(num_facilities / num_cols)  # Round up to the nearest integer\n",
    "\n",
    "# Create subplots with calculated rows and columns\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(18, num_rows * 5), sharex=True)\n",
    "\n",
    "# Iterate over each facility name and its corresponding axis\n",
    "for idx, (facility_name, ax) in enumerate(zip(facility_names, axes.flatten())):\n",
    "    # Filter data for the current facility\n",
    "    facility_data = final_df[final_df['facility_name'] == facility_name]\n",
    "    \n",
    "    # Group the data by month and calculate the average parking_availability total\n",
    "    facility_month_avg = facility_data.groupby('month')['parking_availability'].mean().reset_index()\n",
    "    \n",
    "    # Create bar plot for the current facility\n",
    "    sns.barplot(data=facility_month_avg, x='month', y='parking_availability', palette='viridis', ax=ax)\n",
    "    \n",
    "    # Set title and labels for the current subplot\n",
    "    ax.set_title(f'Average Parking Availability for {facility_name}')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Average Parking Availability')\n",
    "    ax.set_xticks(range(6))\n",
    "    ax.set_xticklabels(['July', 'August', 'September', 'October', 'November', 'December'], rotation=45)\n",
    "    ax.tick_params(axis='x', labelrotation=45)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Hide empty subplots if the number of facilities is not a multiple of num_cols\n",
    "if num_facilities % num_cols != 0:\n",
    "    for ax in axes.flatten()[num_facilities:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above illustrates the monthly average parking availability for each facility, spanning from July through November. \n",
    "\n",
    "**Graph structure**:\n",
    "- x-axis represents months from Jult through December\n",
    "- y-axis indicates the average parking availability for the corresponding facility during that month. \n",
    "\n",
    "There is a noticeable increase in parking availability during December compared to the preceding months. This trend suggests that December generally exhibits higher parking availability across facilities compared to the earlier months. Possible reasons for this could include reduced demand for parking due to holidays, vacations, or changes in usage patterns during the festive season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Average Parking Availability By Day of the Week**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of days of the week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# First subplot: Average Parking Availability by Day of the Week\n",
    "plt.figure(figsize=(10, 5))  # Adjust the figure size as needed\n",
    "sns.barplot(x='day_of_week', y='parking_availability', data=final_df, palette='deep', order=day_order)\n",
    "plt.title('Average Parking Availability by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Parking Availability')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the first plot\n",
    "plt.show()\n",
    "\n",
    "# Second subplot: Parking Availability by Day of the Week per Parking Facility\n",
    "plt.figure(figsize=(20, 10))  # Adjust the figure size as needed\n",
    "sns.barplot(x='day_of_week', y='parking_availability', data=final_df, palette='viridis', hue='facility_id', order=day_order)\n",
    "plt.title('Parking Availability by Day of the Week per Parking Facility')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Parking Availability')\n",
    "plt.legend(title='Parking Facility', loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the second plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average Parking Availability by Day of the Week**.\n",
    "\n",
    "**Graph Structure**:\n",
    "- The x-axis represents the \"Day of the Week,\" featuring days from Monday to Sunday.\n",
    "- The y-axis is labeled as \"**Average Parking Availability**\" and ranges from 0 to 500.\n",
    "Each day of the week has a distinct colored bar that reaches up to approximately 500, indicating high parking availability throughout the week.\n",
    "\n",
    "\n",
    "**Interpretation**:\n",
    "The graph provides insights into the average parking availability for each day of the week. All bars are almost equal in height, suggesting similar parking availability across all days.\n",
    "Despite minor fluctuations, parking availability remains consistent throughout the week.\n",
    "\n",
    "**Parking Availability by Day of the Week per Parking Facility**\n",
    "\n",
    "**Graph Structure**:\n",
    "- The x-axis represents the days of the week, starting from **Monday** and ending with **Sunday**.\n",
    "- The y-axis shows the number of available parking spots, ranging from **0** to approximately **1600**.\n",
    "- Each day has multiple bars (in various colors) representing data for different parking facilities shown by the legend on the right side which indicates the color corresponding to each facility.\n",
    "\n",
    "**Observations**:\n",
    "   - **Saturday** and **Sunday** exhibit some of the highest peaks, indicating high parking availability during these days. Availability varies significantly both between days and across different facilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Parking Availability by Parking Facility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by parking availability in descending order\n",
    "final_df_sorted = final_df.sort_values(by='parking_availability', ascending=False)\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.barplot(y='facility_name', x='parking_availability',\n",
    "            data=final_df_sorted, palette='viridis', orient='h')\n",
    "plt.title('Parking Availability by Parking Facility')\n",
    "plt.xlabel('Parking Availability')\n",
    "plt.ylabel('Parking Facility')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parking Availability by Parking Facility**\n",
    "\n",
    "The graph above provides information about the parking availability at various car parks. Here are the details:\n",
    "\n",
    "**Graph Structure**:\n",
    "- The x-axis represents the \"Parking Availability,\" ranging from 0 to approximately 1400 available spaces.\n",
    "- The y-axis lists the names of different car parks.\n",
    "Each bar corresponds to a specific car park and indicates the number of available parking spaces.\n",
    "\n",
    "**Car Parks and Availability**:\n",
    "   - **Leppington Car Park** and **Edmondson Park South Car Park** have the longest bars, suggesting they have the most available spaces.\n",
    "   - Other car parks, such as **Penrith Combeewood Multi-Level**, **Gosford**, **Kellyville South**, **Warwick Farm**, **Revesby**, and **Bella Vista**, also show significant availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Parking Lot Capacity vs. Occupancy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plotting the line plot on the first subplot\n",
    "sns.lineplot(x='facility_id', y='capacity', data=final_df, label='Capacity', marker='o', ax=axes[0])\n",
    "sns.lineplot(x='facility_id', y='occupancy', data=final_df, label='Occupancy', marker='o', ax=axes[0])\n",
    "\n",
    "# Adding labels and title to the first subplot\n",
    "axes[0].set_xlabel('Parking Lots')\n",
    "axes[0].set_ylabel('Number of Parking Spots')\n",
    "axes[0].set_title('Parking Lot Capacity vs. Occupancy')\n",
    "axes[0].legend()\n",
    "\n",
    "# Melt the DataFrame for Seaborn\n",
    "df_melted = pd.melt(final_df[['facility_name', 'capacity', 'occupancy']], id_vars='facility_name', var_name='Category', value_name='Value')\n",
    "\n",
    "# Creating a multiple bar plot on the second subplot\n",
    "sns.barplot(y='facility_name', x='Value', hue='Category', data=df_melted, palette='muted', orient='h', ax=axes[1])\n",
    "\n",
    "# Adding labels and title to the second subplot\n",
    "axes[1].set_ylabel('Parking Facilities')\n",
    "axes[1].set_xlabel('Number of Parking Spots')\n",
    "axes[1].set_title('Parking Lot Capacity vs. Occupancy')\n",
    "axes[1].legend(title='Category')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Line Chart**:\n",
    "- **Blue Line (Capacity)**: Represents the total number of parking spots available (capacity).\n",
    "- **Orange Line (Occupancy)**: Indicates the actual car occupancy in the parking lots.\n",
    "Both lines fluctuate over time and generally, the orange line (occupancy) remains below the blue line (capacity), suggesting available spaces in all lots.\n",
    "\n",
    "\n",
    "**Bar Chart**:\n",
    "The right graph lists specific car parks along with their capacities and occupancies and each bar represents a parking facility:\n",
    "- **Blue Bars**: Show the total capacity (number of parking spots).\n",
    "- **Orange Bars**: Represent the current occupancy levels.\n",
    "Observations:\n",
    "- Most car parks have available spaces (difference between blue and orange bars) and some car parks are nearly full.\n",
    "    \n",
    "\n",
    "**Recommendations**:\n",
    "- **Underutilized Lots**:\n",
    "For car parks with consistently low occupancy, consider:\n",
    "    - Implementing dynamic pricing to encourage use during off-peak hours.\n",
    "    - Explore partnerships (with nearby businesses) to increase utilization.\n",
    "\n",
    "- **Highly Occupied Lots**:\n",
    "For lots nearing full capacity, consider:\n",
    "    - Expanding or optimizing management systems to prevent overflow.\n",
    "    - Offering alternative transportation options (shuttles, public transit) to reduce reliance on personal vehicles.\n",
    "    - Implementing real-time availability updates to guide drivers to less crowded lots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Parking Availability by Time Category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot grouped by time category\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# Order of time category\n",
    "time_order = ['Early Morning', 'Morning', 'Afternoon', 'Late Afternoon', 'Evening', 'Night']\n",
    "sns.barplot(x='time_category', y='parking_availability',\n",
    "            data=final_df, palette='viridis', order = time_order)\n",
    "plt.title('Parking Availability by Time Category')\n",
    "plt.xlabel('Time Category')\n",
    "plt.ylabel('Parking Availability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parking Availability by time category**\n",
    "\n",
    "**Graph Structure**:\n",
    "- The x-axis represents different **time categories**\n",
    "- The y-axis represents the **number of available parking spots** and the vertical bars indicate the parking availability for each time category.\n",
    "\n",
    "The graph illustrates the distribution of parking availability across different time categories. Early morning and night exhibit the highest availability, with approximately 700 spots each, followed by evening with around 600 spots. Morning and late afternoon maintain relatively stable availability, with about 300 spots each. The observations suggest potential opportunities for optimizing parking management strategies, such as dynamic pricing during peak hours and providing real-time availability updates to drivers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**Multivariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Effect of Holiday on Parking Availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot with Seaborn\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(x='month', y='parking_availability', hue='is_holiday', data=final_df, palette='Set2', estimator='mean')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Average Parking Availability: Holidays vs Non-Holidays (July to December)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Parking Availability')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Display the legend\n",
    "plt.legend(title='Holiday Status')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average Parking Availability: Holidays vs Non-Holidays (July to December)**\n",
    "\n",
    "**Graph Structure**:\n",
    "- The title of the graph is \"**Average Parking Availability: Holidays vs Non-Holidays (July to December)**.\"\n",
    "- The x-axis represents the months from **July** to **December** with two sets of bars for each month:\n",
    "    - **Orange Bars**: Represent holidays.a\n",
    "    - **Green Bars**: Represent non-holidays.\n",
    "- The y-axis represents the **average parking availability**, ranging from 0 to slightly above 600.\n",
    "\n",
    "**Observations**:\n",
    "- For most months (July to November), parking availability is higher during **non-holidays**. However, in **December**, parking availability is higher during **holidays**. This observation suggests the need to adjust parking management strategies to accommodate holiday schedules, optimize resources during peak holiday periods, and continually monitor trends to ensure sufficient availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Correlation Heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only numeric columns\n",
    "numeric_columns = ['parking_availability', 'capacity', 'occupancy',  'longitude', 'latitude']\n",
    "\n",
    "# Select numeric columns from the DataFrame\n",
    "numeric_data = final_df[numeric_columns]\n",
    "\n",
    "# Create a correlation matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "# Set the title\n",
    "plt.title('Correlation Heatmap for Numeric Columns')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown:\n",
    "\n",
    "**Graph Structure**:\n",
    "- The x-axis and y-axis list the following variables:\n",
    "    - **parking_availability**\n",
    "    - **capacity**\n",
    "    - **occupancy**\n",
    "    - **longitude**\n",
    "    - **latitude**\n",
    "\n",
    "- The heatmap colors indicate the strength of correlation:\n",
    "    - Red (1): Strong positive correlation\n",
    "    - Blue (-0.2): Negative correlation\n",
    "\n",
    "The heatmap illustrates correlations between various factors in the dataset. Strong positive correlations exist between parking availability and capacity, as well as between occupancy and capacity. Additionally, latitude and longitude exhibit a moderate positive correlation. Notably, parking availability shows a negative correlation with latitude. These insights can guide parking management decisions, indicating that areas with larger capacity tend to have higher availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table for heatmap\n",
    "heatmap_data = final_df.pivot_table(\n",
    "    values='parking_availability', index='day_of_week', columns='time_category', aggfunc='mean')\n",
    "\n",
    "# Define the order of days of the week\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Define the order of time categories\n",
    "time_order = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(heatmap_data.loc[days_order, time_order], annot=True, cmap='viridis', fmt='.1f', linewidths=.5)\n",
    "plt.title('Overall Parking Availability Trends')\n",
    "plt.xlabel('Time Category')\n",
    "plt.ylabel('Day of the Week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Time Series Decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# # Assuming final_df contains the data with the 'date' column as the index\n",
    "\n",
    "# # List of unique facility names\n",
    "# facility_names = final_df['facility_name'].unique()\n",
    "\n",
    "# # Set the 'date' column as the index for the entire DataFrame\n",
    "# final_df.set_index('date', inplace=True)\n",
    "\n",
    "# # Iterate through each facility\n",
    "# for facility_name in facility_names:\n",
    "#     # Filter data for the current facility\n",
    "#     facility_data = final_df[final_df['facility_name'] == facility_name]\n",
    "    \n",
    "#     # Resample the data to daily frequency and fill missing values\n",
    "#     facility_data = facility_data.resample('D').mean().fillna(method='ffill')\n",
    "    \n",
    "#     # Perform seasonal decomposition\n",
    "#     decomposition = seasonal_decompose(facility_data['parking_availability'], model='additive', period=7)  # Assuming weekly seasonality\n",
    "    \n",
    "#     # Plot the decomposition results\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.suptitle(f\"Seasonal Decomposition for {facility_name}\", fontsize=16)\n",
    "\n",
    "#     plt.subplot(411)\n",
    "#     plt.plot(facility_data['parking_availability'], label='Original')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.subplot(412)\n",
    "#     plt.plot(decomposition.trend, label='Trend')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.subplot(413)\n",
    "#     plt.plot(decomposition.seasonal, label='Seasonality')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.subplot(414)\n",
    "#     plt.plot(decomposition.resid, label='Residuals')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delve deeper into the time series decomposition graph and explore each component:\n",
    "\n",
    "1. **Original Component**:\n",
    "   - The **original component** represents the **raw data** without any adjustments. In this case, it appears to be a dataset that varies over time.\n",
    "   - Looking at the graph, we notice several oscillations (ups and downs) along with an overall declining trend. These fluctuations could be due to various factors, such as seasonality, external events, or underlying patterns.\n",
    "\n",
    "2. **Trend Component**:\n",
    "   - The **trend component** is obtained by **smoothing out** the original data to reveal the **long-term behavior** or direction.\n",
    "   - In your graph, the trend shows a **gradual decline** over the observed period (from July 2023 to January 2024). This decline might indicate a consistent downward movement in the underlying phenomenon being measured.\n",
    "\n",
    "3. **Seasonality Component**:\n",
    "   - The **seasonality component** captures **recurring patterns** that occur at regular intervals (e.g., daily, monthly, or yearly).\n",
    "   - From the graph, we can see periodic peaks and valleys. These could correspond to seasonal effects, such as monthly fluctuations, holidays, or other cyclic events.\n",
    "\n",
    "4. **Residuals Component**:\n",
    "   - The **residuals component** represents the **unexplained variations** in the data after removing the trend and seasonality.\n",
    "   - These residuals are essentially the **noise** or irregularities left behind. They could result from random fluctuations, measurement errors, or other factors not accounted for by the trend and seasonality.\n",
    "\n",
    "**Conclusion**:\n",
    "- The declining trend suggests that there might be an underlying reason for the overall decrease in the observed phenomenon.\n",
    "- Seasonality could provide insights into recurring patterns. For example, if this data represents sales, the peaks might coincide with holiday seasons.\n",
    "- Analyzing the residuals can help identify anomalies or unexpected deviations from the expected behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Next Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To implement a real-time data acquisition pipeline to continuously update the model with the latest parking occupancy data.\n",
    "\n",
    "2. To conduct pilot tests and gather feedback from end-users to assess the usability and effectiveness of the parking prediction system in real-world scenarios.\n",
    "\n",
    "3. To deploy the finalized parking spot predictor in urban areas, collaborating with city authorities and parking management companies to integrate it into existing infrastructure and promote widespread adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, in order to ensure that the changes are carried over to the modelling, we will assign the dataframe to the DataModelling object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis.carpark_history = final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off by instantiating a model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = Modelling(data_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off, a new variable `data` will be used to represent the dataframe being used to create the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/modelling_data_1_month.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddata = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(df):\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"date\"].astype(\"str\") + ' ' + df[\"time\"].astype(\"str\"), format='%Y-%m-%d %H:%M:%S')\n",
    "    return df\n",
    "\n",
    "modelling_data = g(data)\n",
    "#setting timestamp as the index\n",
    "modelling_data.set_index(\"timestamp\",  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dropping columns we dont need\n",
    "# # modelling_data.drop([\"ParkID\", \"zone_id\", \"date\", \"time\"], axis= 1,inplace= True)\n",
    "# modelling_data.drop([\"date\", \"time\"], axis= 1,inplace= True)\n",
    "# modelling_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. ARIMA model analysis**\n",
    "\n",
    "Doing analysis of trends using rolling means of different sizes, for instance daily,weekly and monthly. We are going to analyze our target variable only to understand its behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_data = modelling_data.copy()\n",
    "# plotting_data.drop(columns=['facility_name', 'total_parking_spots', 'occupancy_total',\n",
    "#     'day_of_week', 'time_category', 'z_score'], inplace=True, axis=1)\n",
    "plotting_data = plotting_data[['parking_availability']]\n",
    "#inspecting df\n",
    "plotting_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the total `parking availability` for time and day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the parking availability trend\n",
    "plotting_data.plot(figsize = (15, 8))\n",
    "plt.title(\"parking pattern over time\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"total_parking_availability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our time series data has high frequency or irregular time intervals, we considered resampling to a lower frequency to obtain a more meaningful plot. Resampling allows you to aggregate data over specified time intervals, in our case we shall resample the data down to day, week and month\n",
    "\n",
    "We wrote a function to help us plot the patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for plotting patterns\n",
    "def parking_pattern(data, size):\n",
    "    data.plot(figsize = (15, 8))\n",
    "    plt.title(f\"parking pattern on a {size} basis\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(\"total_parking_availability\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Daily pattern of parking availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pattern = plotting_data.resample(\"D\").mean()\n",
    "parking_pattern(daily_pattern, \"daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a trend on the daily pattern of parking  spaces in cities. The number of available parking spots remains at a 100 for almost 5 months and shoots up almost to 600 during december."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Weekly pattern of parking availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_pattern = plotting_data.resample(\"W\").mean()\n",
    "parking_pattern(weekly_pattern, \"weekly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weekly pattern is almost straight except for the last month when it rises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Monthly parking availability pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_pattern = plotting_data.resample(\"M\").mean()\n",
    "parking_pattern(monthly_pattern, \"monthly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, parkin availability remains low over the months and increases in the final month. This is evident in all plots and can indicate that in the month of december people stay away from towns for the holidays and some visit upcountry to visit their friends or family\n",
    "\n",
    "**Stationarity**\n",
    "\n",
    "Checking for stationarity. A time series is considered stationary if its statistical properties, such as mean, variance, and autocorrelation, do not change over time. In simple words, the time series data shows consistent behavior and does not exhibit trends or seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing adfuller test from statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def stationarity_test(dataset):\n",
    "    ad_test = adfuller(dataset, autolag = \"AIC\")\n",
    "    print(\"1. ADF: \",ad_test[0])\n",
    "    print(\"2. p-value: \",ad_test[1])\n",
    "    print(\"3. Number of lags: \", ad_test[2])\n",
    "    print(\"4. Number of obeservations used for ADF regression and critical values calculation: \", ad_test[3])\n",
    "    print(\"5. Critical values: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiating the `autoarima model` for our time series problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pmdarima import auto_arima\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# #fitting the model\n",
    "# auto_arima_model = auto_arima(plotting_data[\"parking_availability\"], trace= True, suppress_warnings= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we already know the behavior of our data, the model to use and and the order to specify we will go ahead and train a model and make some few predictions. We are also going to split our data into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data\n",
    "train_data, test_data = train_test_split(daily_pattern, test_size= 0.3, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating ARIMA model\n",
    "arima_model = ARIMA(train_data, order = (2, 1, 2))\n",
    "arima_model = arima_model.fit()\n",
    "arima_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also do a prediction to see how well our model has learnt. For the predict method, you need to pass in the start and end parameters for the period you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying start and end\n",
    "start = len(train_data)\n",
    "end = len(train_data) + len(test_data)-1\n",
    "predictions = arima_model.predict(start = start, end= end, type = \"level\")\n",
    "predictions.index = plotting_data.index[start:end+1]\n",
    "print(round(predictions, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.plot(legend =True)\n",
    "# daily_pattern[\"parking_availability\"].plot(legend= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(daily_pattern.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#getting the mse and rmse\n",
    "pred_mse = (mean_squared_error(predictions, test_data))\n",
    "pred_rmse = sqrt(mean_squared_error(predictions, test_data))\n",
    "pred_mae = (mean_absolute_error(predictions, test_data))\n",
    "\n",
    "print(f\"The MSE score is {pred_mse}\")\n",
    "print(f\"The RMSE score is {pred_rmse}\")\n",
    "print(f\"The MAE score is: {pred_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rmse range is greater than the mean of the data, meaning that for every prediction we make we are getting a value 160.2 further away from the actual result. This means that our model is not performing well and needs improvement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Making Future Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to take place effectively we will train the model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model2 = ARIMA(plotting_data, order= (2,1,2))\n",
    "arima_model2 = arima_model2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up future dates \n",
    "future_dates = pd.date_range(start= \"2024-01-01\", end = \"2024-01-31\")\n",
    "#making prediction\n",
    "future_preds = arima_model2.predict(start = len(plotting_data), end = len(plotting_data)+ 30, type = \"levels\")\n",
    "future_preds.index = future_dates\n",
    "print(future_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the predictions from the ARIMA model, we can see there is similarity in the values except for the decimal values. This indicates our model has not captured most of the seasonality and trend component.Our model maybe also overfitting to the training data or it may be that the series does not have a strong enough seasonal component.\n",
    "\n",
    "Having also many facilities makes it hard to interpret the prediction for each facility. To overcome this problem, we are going to group our data using facility name and then model time series for each facility in our dataset. This will ease prediction of a parking availability for a given facility, date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = dddata.copy()\n",
    "grouped_data.drop(columns=[\"ParkID\", \"zone_id\"], axis= 1,inplace= True)\n",
    "\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. PROPHET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After grouping our data, we are going to an advanced machine learning algorithm called `Prophet`. Prophet is an open-source forecasting tool developed by Facebook for time series analysis. It is designed to handle daily observations that display patterns such as seasonality and holidays. Prophet simplifies the forecasting process and provides intuitive methods for producing accurate predictions with customizable components.\n",
    "\n",
    "We will create a class for the `Prophet` that iterates over grouped data does time series using given columns and allows us to make predictions  on future data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert to datetime\n",
    "def to_datetime(x):\n",
    "    return datetime.strptime(x,  \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "pprophet_data = pd.read_csv(\"modelling_data.csv\", parse_dates= [[\"date\", \"time\"]], date_parser= to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting datetime as index\n",
    "prophet_data = pprophet_data.drop([\"day_of_week\", \"time_category\", \"ParkID\", \"zone_id\", \"z_score\"], axis= 1)\n",
    "prophet_data = prophet_data.set_index(\"date_time\").groupby(\"facility_name\").resample(\"D\").mean()\n",
    "\n",
    "#inspecting our data\n",
    "prophet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two index columns in our grouped dataframe, we will reset the index, return date_time as the index and plot and do some visualizations on our facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting index\n",
    "prophet_data = prophet_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting all trends in one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_facilities(data, facilities, columns=[\"total_parking_spots\", \"occupancy_total\", \"parking_availability\"], figsize=(15, 5)):\n",
    "    # Set the 'date_time' column as the index\n",
    "    data = data.set_index(\"date_time\")\n",
    "\n",
    "    # Filter data for the specified facilities\n",
    "    filtered_data = data[data['facility_name'].isin(facilities)]\n",
    "\n",
    "    for facility in facilities:\n",
    "        facility_data = filtered_data.query(f\"facility_name == '{facility}'\")\n",
    "\n",
    "        # Create a new figure for each facility\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot the data for the current facility\n",
    "        facility_data[columns].plot(ax=ax, label=facility)\n",
    "\n",
    "        # Set title, labels, and legend\n",
    "        ax.set_title(f\"Time Series Data for {facility}\")\n",
    "        ax.set_xlabel(\"Date and Time\")\n",
    "        ax.set_ylabel(\"Values\")\n",
    "        ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "facilities_to_plot = [\"Bella Vista Car Park\", \"West Ryde Car Park\", \"Narrabeen Car Park\", \"Campbelltown Farrow Rd North Car Park \" ,\n",
    "                    \"Warwick Farm Car Park\", \"Warriewood Car Park\", \"Tallawong P3 Car Park \", \"Tallawong P3 Car Park\", \"Tallawong P2 Car Park\",\n",
    "                    \"Tallawong P1 Car Park\", \"Sutherland East Parade Car Park\", \"St Marys Car Park\", \"Schofields Car Park \", \"Revesby Car Park\",\n",
    "                    \"Penrith Combewood Multi-Level Car Park\", \"Penrith Combewood At-Grade Car Park\", \"Leppington Car Park\", \"Kiama Car Park \",\n",
    "                    \"Kellyville South Car Park\", \"Kellyville North Car Park\", \"Hornsby Jersey St Car Park\", \"Hills Showground Car Park\", \"Gosford Car Park \",\n",
    "                    \"Gordon Henry St North Car Park\", \"Edmondson Park South Car Park\", \"Dee Why Car Park\", \"Cherrybrook Car Park\", \"Campbelltown Hurley Street South Car Park \"]\n",
    "\n",
    "#calling the plot function\n",
    "plot_facilities(prophet_data, facilities_to_plot, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! We can see our parking facilities have different seasonalities and trends. We can not generalize this and build one time series model. What we will do is to create separate models for each of the facility, which will allow us to make accurate predictions and forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Prophet Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet is a time series forecasting tool developed by Facebook that utilizes an additive model to capture seasonality, trends, and holidays, providing a user-friendly approach for accurate and efficient predictions. # We will create a final dataframe that has the date_time column and the columns we will need for our analysis. Prophet model expects data in a particular format; it expects the name of date column to be \"ds\" and the target variable as y # Before everything we will create a prophet pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a pipeline\n",
    "prophet_pipeline = Pipeline(steps=[\n",
    "    (\"prophet\", Prophet(interval_width = 0.95))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prophet_data = prophet_data[[\"facility_name\", \"date_time\",\n",
    "                    \"occupancy_total\", \"parking_availability\"]].rename({\"date_time\": \"ds\", \"parking_availability\": \"y\"}, axis = \"columns\")\n",
    "final_prophet_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping by facility names to allow us to know all the stations and model different time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facility_names = final_prophet_data.groupby(\"facility_name\")\n",
    "facility_names.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an empty dataframe so we can build a forecast for the whole 28 facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty dataframe\n",
    "target = pd.DataFrame()\n",
    "\n",
    "# Looping over facilities to perform forecasting\n",
    "for facility in facility_names.groups:\n",
    "    group = facility_names.get_group(facility)\n",
    "    model = Prophet(interval_width=0.95)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(group)\n",
    "    \n",
    "    # Create a future DataFrame\n",
    "    future = model.make_future_dataframe(periods=31)\n",
    "    \n",
    "    # Generate forecasts\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # Plot the forecast with title\n",
    "    fig = model.plot(forecast)\n",
    "    fig.suptitle(f\"Forecast for Facility: {facility}\", y=1.02, fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics using the first 25 values of actual data\n",
    "    actual_values = group['y'][:25].values\n",
    "    predicted_values = forecast[:25]['yhat'].values\n",
    "\n",
    "    if not np.isnan(actual_values).any() and not np.isnan(predicted_values).any():\n",
    "        mse = mean_squared_error(actual_values, predicted_values)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(actual_values, predicted_values)\n",
    "    else:\n",
    "        mse = np.nan\n",
    "        rmse = np.nan\n",
    "        mae = np.nan\n",
    "    # Print metrics for each facility\n",
    "    print(f\"Metrics for Facility: {facility}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Rename columns in the forecast DataFrame\n",
    "    forecast = forecast.rename(columns={\"yhat\": f\"yhat_{facility}\", \"ds\": \"ds_forecast\"})\n",
    "    \n",
    "    # Drop unnecessary columns before merging\n",
    "    forecast = forecast[[\"ds_forecast\", f\"yhat_{facility}\"]]\n",
    "    \n",
    "    # Merge with updated column names\n",
    "    target = pd.merge(target, forecast.set_index(\"ds_forecast\"), how=\"outer\", left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blackpoints on the plots are the actual `parking_availability` and the blue points  are the predicted values. The light blue background  represents the confidence interval which in our case is set to 95%.\n",
    "\n",
    "The dataframe above contains the forecast of the next 1 month based on the previous data the model has learned. We will plot the actual values versus the predicted values to see how  well our model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facilities names\n",
    "facility_names = [\"Bella Vista Car Park\", \"West Ryde Car Park\", \"Narrabeen Car Park\",\n",
    "                    \"Warwick Farm Car Park\", \"Warriewood Car Park\",\"Tallawong P2 Car Park\",\n",
    "                    \"Tallawong P1 Car Park\", \"Sutherland East Parade Car Park\", \"St Marys Car Park\", \"Revesby Car Park\",\n",
    "                    \"Penrith Combewood Multi-Level Car Park\", \"Penrith Combewood At-Grade Car Park\", \"Leppington Car Park\",\n",
    "                    \"Kellyville South Car Park\", \"Kellyville North Car Park\", \"Hornsby Jersey St Car Park\", \"Hills Showground Car Park\",\n",
    "                    \"Gordon Henry St North Car Park\", \"Edmondson Park South Car Park\", \"Dee Why Car Park\", \"Cherrybrook Car Park\"]\n",
    "\n",
    "# Loop through facility names\n",
    "for facility in facility_names:\n",
    "    # Get actual data for the facility from the original dataset\n",
    "    actual_data = final_prophet_data.set_index(\"ds\").query(f\"facility_name == '{facility}'\")[\"y\"]\n",
    "\n",
    "    # Get predicted data for the facility from the target DataFrame\n",
    "    predicted_data = target[f\"yhat_{facility}\"]\n",
    "\n",
    "    # Concatenate actual and predicted data\n",
    "    combined_data = pd.concat([actual_data, predicted_data], axis=1)\n",
    "\n",
    "    # Plot the actual vs predicted data \n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    combined_data.plot(ax=ax, title=f\"Actual vs Predicted for {facility}\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Values\")\n",
    "    ax.legend([\"Actual\", \"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots we can see our model recognizes the seasonality of our data although it doesn't recognize the peaks and this can suggest doing a multivariate analysis to incorporate other exogenous(external)  variables such as holidays and weather to see if our model will recognize the peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics used to evaluate timeseries  forecasting models are: MAE (Mean Absolute Error), RMSE (Root Mean Squared Error) and MAPE (Mean Absolute Percentage Error). We will use these metrics to evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Multivariate Prophet Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in our initial model that the peaks couldn't be recognized when making predictions, we will go ahead and build another model that incorporates other external variables to see if the model will improve on realising the points it didn't recognize earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our original dataframe\n",
    "pprophet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting datetime as index\n",
    "second_prophet_data = pprophet_data.drop([\"ParkID\", \"zone_id\", \"z_score\", \"time_category\"], axis= 1)\n",
    "#getting dummies\n",
    "dummies_df = pd.get_dummies(second_prophet_data[['day_of_week']],\n",
    "                            columns=[\"day_of_week\"], prefix= [\"day\"])\n",
    "#converting dummies to numerical\n",
    "dummies_df = dummies_df.astype(int)\n",
    "#joining the dummies and original df\n",
    "final_df = pd.concat( [second_prophet_data, dummies_df] ,axis=1)\n",
    "#dropping cat columns\n",
    "final_df.drop( columns = \"day_of_week\", axis= 1, inplace = True)\n",
    "\n",
    "#inspecting our final df\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all columns in a format we can work with in machine learning.\n",
    "\n",
    "Lets resample the data into daily observations for easy handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting date to datetime\n",
    "final_df['date_time'] = pd.to_datetime(final_df['date_time'])\n",
    "\n",
    "# Set the datetime as index\n",
    "final_df.set_index('date_time', inplace=True)\n",
    "#resampling our data\n",
    "final_data = final_df.groupby(\"facility_name\").resample(\"D\").mean()\n",
    "#reset index\n",
    "final_data = final_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming our columns in order to fit the prophet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns\n",
    "col_mapper  = {\"date_time\": \"ds\", \"parking_availability\": \"y\"}\n",
    "#rename\n",
    "final_data.rename(columns=col_mapper, inplace=True)\n",
    "#inspecting\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating prophet model so we can start modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping by facility in order to get predictions for each\n",
    "parking_lots = final_data.groupby(\"facility_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty dataframe\n",
    "second_target = pd.DataFrame()\n",
    "\n",
    "# Looping over facilities to perform forecasting\n",
    "for facility in parking_lots.groups:\n",
    "    group = parking_lots.get_group(facility)\n",
    "    \n",
    "    # Instantiate the model\n",
    "    prophet_model = Prophet(interval_width=0.95, daily_seasonality= False)\n",
    "    \n",
    "    #Adding regressors to the model\n",
    "    for col in dummies_df.columns:\n",
    "        prophet_model.add_regressor(col)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    prophet_model.fit(group)\n",
    "    \n",
    "    # Create a future DataFrame with regressors\n",
    "    future = prophet_model.make_future_dataframe(periods=31)\n",
    "    \n",
    "    for col in dummies_df.columns:\n",
    "        future[col] = dummies_df[col]\n",
    "    \n",
    "    # Generate forecasts\n",
    "    forecast = prophet_model.predict(future)\n",
    "    final_data\n",
    "    # Plot the forecast\n",
    "    prophet_model.plot(forecast)\n",
    "    \n",
    "    # Rename columns in the forecast DataFrame\n",
    "    forecast = forecast.rename(columns={\"yhat\": f\"yhat_{facility}\", \"ds\": \"ds_forecast\"})\n",
    "    \n",
    "    # Drop unnecessary columns before merging\n",
    "    forecast = forecast[[\"ds_forecast\", f\"yhat_{facility}\"]]\n",
    "    \n",
    "    # Merge with updated column names\n",
    "    second_target = pd.merge(second_target, forecast.set_index(\"ds_forecast\"), how=\"outer\", left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting actual versus predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facilities names\n",
    "facility_names = [\"Bella Vista Car Park\", \"West Ryde Car Park\", \"Narrabeen Car Park\",\n",
    "                    \"Warwick Farm Car Park\", \"Warriewood Car Park\",\"Tallawong P2 Car Park\",\n",
    "                    \"Tallawong P1 Car Park\", \"Sutherland East Parade Car Park\", \"St Marys Car Park\", \"Revesby Car Park\",\n",
    "                    \"Penrith Combewood Multi-Level Car Park\", \"Penrith Combewood At-Grade Car Park\", \"Leppington Car Park\",\n",
    "                    \"Kellyville South Car Park\", \"Kellyville North Car Park\", \"Hornsby Jersey St Car Park\", \"Hills Showground Car Park\",\n",
    "                    \"Gordon Henry St North Car Park\", \"Edmondson Park South Car Park\", \"Dee Why Car Park\", \"Cherrybrook Car Park\"]\n",
    "\n",
    "# Loop through facility names\n",
    "for facility in facility_names:\n",
    "    # Get actual data for the facility from the original dataset\n",
    "    actual_data = final_data.set_index(\"ds\").query(f\"facility_name == '{facility}'\")[\"y\"]\n",
    "\n",
    "    # Get predicted data for the facility from the target DataFrame\n",
    "    predicted_data = second_target[f\"yhat_{facility}\"]\n",
    "\n",
    "    # Concatenate actual and predicted data\n",
    "    combined_data = pd.concat([actual_data, predicted_data], axis=1)\n",
    "\n",
    "    # Plot the actual vs predicted data \n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    combined_data.plot(ax=ax, title=f\"Actual vs Predicted for {facility}\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Values\")\n",
    "    ax.legend([\"Actual\", \"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse = mean_squared_error(final_data['y'], forecast[\"yhat\"][-len(actual_data):])\n",
    "# rmse = np.sqrt(mse)\n",
    "# mae = mean_absolute_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST for Time Series\n",
    "XGBoost for time series involves transforming time-related features and historical observations into a structured input for the model. Incorporating relevant temporal information can enhance its ability to capture patterns, trends, and seasonality, improving forecasting accuracy. Experimentation with feature engineering and model parameters is crucial for optimizing performance in time series applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our initial dataframe\n",
    "modelling_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since XGBOOST enjoys working with incorporated relevant temporal features, we are going to feature engineer our `timestamp`  column by creating new columns that capture the day of the week the hour of the day and minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting index\n",
    "xgboost_data = modelling_data.reset_index()\n",
    "#inspecting the df\n",
    "print(xgboost_data.info())\n",
    "print(xgboost_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_dummies = pd.get_dummies(xgboost_data[['day_of_week', 'time_category']],\n",
    "                            columns=[\"day_of_week\", \"time_category\"], prefix= [\"day_of_the_week\", \"time_period\"],\n",
    "                            sparse= False, drop_first= True)\n",
    "#converting dummies to numerical\n",
    "xg_dummies = xg_dummies.astype(int)\n",
    "#joining the dummies and original df\n",
    "xg_encoded = pd.concat( [xgboost_data, xg_dummies] ,axis=1)\n",
    "#dropping cat columns\n",
    "xg_encode = xg_encoded.drop([\"time_category\", \"day_of_week\", \"z_score\"], axis= 1)\n",
    "xg_encoded = xg_encoded.set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_data =  xgboost_data.drop([\"day_of_week\", \"time_category\", \"z_score\"], axis= 1)\n",
    "vanilla_data = vanilla_data.set_index(\"timestamp\")\n",
    "\n",
    "xg_grouped = vanilla_data.groupby(\"facility_name\")\n",
    "\n",
    "# Dictionary to store XGBoost models for each parking lot\n",
    "xg_models = {}\n",
    "\n",
    "# Train a model for each parking lot\n",
    "for facility_name, group_data in xg_grouped:\n",
    "    # Split the data into features and target variable\n",
    "    X = group_data.drop(columns=[\"parking_availability\", \"facility_name\"], axis=1)\n",
    "    y = group_data[\"parking_availability\"]\n",
    "\n",
    "    # Create an XGBoost model\n",
    "    xg_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "    # Train the model\n",
    "    xg_model.fit(X, y)\n",
    "\n",
    "    # Make predictions on the entire dataset\n",
    "    group_data[\"prediction\"] = xg_model.predict(X)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y, group_data[\"prediction\"])\n",
    "    rmse = sqrt(mse)\n",
    "    mae = mean_absolute_error(y,group_data[\"prediction\"])\n",
    "    print(f'Mean Squared Error for {facility_name}: {mse}')\n",
    "    print(f'Root Mean Squared Error for {facility_name}: {rmse}')\n",
    "    print(f'Mean Absolute Error for {facility_name}: {mae}')\n",
    "    # Store the trained model in the dictionary\n",
    "    xg_models[facility_name] = xg_model\n",
    "\n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(group_data.index, y, label='Actual', marker='o')\n",
    "    plt.plot(group_data.index, group_data[\"prediction\"], label='Predicted', marker='o')\n",
    "    plt.title(f'Actual vs Predicted for {facility_name}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Parking Availability')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is learning too perfectly from the training data such that we cannot see the plot of actual data points because they have been covered by the predicted values.\n",
    "We will add some few other columns and penalties to see if it will improve.\n",
    "\n",
    "Now that we have our best parameters for the Xgboost model. We are going to build final models for each time series and make predictions with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = { \n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"gamma\": [0, 0.1, 0.2],\n",
    "    \"lambda\": [0.01,  0.1],\n",
    "    \"alpha\": [0.01, 0.1, 0.2],\n",
    "    \"n_estimators\": [20, 40, 50]\n",
    "}\n",
    "#class XGBOOST\n",
    "class XGBoostTimeSeriesModel:\n",
    "    def __init__(self, param_grid, target_variable='parking_availability', cv=3, n_jobs=-1):\n",
    "        self.param_grid = param_grid\n",
    "        self.target_variable = target_variable\n",
    "        self.cv = cv\n",
    "        self.n_jobs = n_jobs\n",
    "        self.models = {}\n",
    "\n",
    "    def _get_features_target(self, group_data):\n",
    "        X = group_data.drop(columns=[self.target_variable, \"facility_name\"], axis=1)\n",
    "        y = group_data[self.target_variable]\n",
    "        return X, y\n",
    "\n",
    "    def train_models(self, xg_grouped):\n",
    "        for facility_name, group_data in xg_grouped:\n",
    "            X, y = self._get_features_target(group_data)\n",
    "\n",
    "            # Create an XGBoost model\n",
    "            xg_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "            # Perform GridSearchCV\n",
    "            grid_search = GridSearchCV(xg_model, param_grid=self.param_grid, cv=self.cv, n_jobs=self.n_jobs)\n",
    "            grid_search.fit(X, y)\n",
    "\n",
    "            # Get the best hyperparameters\n",
    "            best_params = grid_search.best_params_\n",
    "            # print(f'Best Hyperparameters for {facility_name}: {best_params}')\n",
    "\n",
    "            # Use the best model from GridSearchCV\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "            # Train the model\n",
    "            best_model.fit(X, y)\n",
    "\n",
    "            # Store the trained model in the dictionary\n",
    "            self.models[facility_name] = best_model\n",
    "\n",
    "    def predict_and_plot(self, xg_grouped):\n",
    "        for facility_name, model in self.models.items():\n",
    "            # Get the last available data for each facility\n",
    "            last_data = xg_grouped.get_group(facility_name).tail(1).drop(columns=[\"parking_availability\", \"facility_name\"])\n",
    "\n",
    "            # Make predictions for the next time period\n",
    "            facility_predictions = model.predict(last_data)\n",
    "\n",
    "            # Plot actual vs predicted values\n",
    "            group_data = xg_grouped.get_group(facility_name)\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.plot(group_data.index, group_data[self.target_variable], label='Actual')\n",
    "            plt.plot(group_data.index, facility_predictions, label='Predicted')\n",
    "            plt.title(f'Actual vs Predicted for {facility_name}')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Parking Availability')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "xg_model = XGBoostTimeSeriesModel(param_grid)\n",
    "xg_model.train_models(xg_grouped)\n",
    "xg_model.predict_and_plot(xg_grouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was deployed on streamlit for use in public. The UI looked as follows:\n",
    "\n",
    "<div>\n",
    "<img src='./images/model1.jpeg' alt='Model ui landing page'/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src='./images/model2.jpeg' alt='Model ui Selected Carpark'/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src='./images/model3.jpeg' alt='Model ui Results'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
