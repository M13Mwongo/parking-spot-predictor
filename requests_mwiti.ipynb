{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import requests\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making requests to API endpoint to retrieve 6 months data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_carpark_history(facility, dates_array):\n",
    "    # Initialize data array\n",
    "    data_array = []\n",
    "\n",
    "    # Define the path for the JSON file\n",
    "    json_file_path = f\"./data/carpark_history_prev_3_months/facility_{facility}.json\"\n",
    "\n",
    "    # Set the request header\n",
    "    headers = {\n",
    "        \"Authorization\": f\"apikey {os.environ.get('apikey')}\"\n",
    "    }\n",
    "\n",
    "    # Delete the file if it exists\n",
    "    if os.path.exists(json_file_path):\n",
    "        os.remove(json_file_path)\n",
    "\n",
    "    # Make a request for each date and aggregate the data\n",
    "    for date in dates_array:\n",
    "        url = f'https://api.transport.nsw.gov.au/v1/carpark/history?facility={facility}&eventdate={date}'\n",
    "        response = requests.get(url, headers=headers).json()\n",
    "\n",
    "        if data_array == []:\n",
    "            data_array = response\n",
    "        else:\n",
    "            data_array = data_array + response\n",
    "\n",
    "    # Save the data to a parquet file\n",
    "    pd.DataFrame(data_array).to_parquet(json_file_path)\n",
    "\n",
    "    # Give feedback on status\n",
    "    print(f\"Data for facility {facility} saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_getter(td):\n",
    "    # Array that stores the dates to be searched for\n",
    "    date_period_list = []\n",
    "\n",
    "    # The last date to be searched for\n",
    "    cutoff_date = datetime(2023, 9, 30)\n",
    "    target_date = cutoff_date - td\n",
    "\n",
    "    # Ensure that records of each day are obtained\n",
    "    delta = timedelta(days=1)\n",
    "\n",
    "    while target_date <= cutoff_date:\n",
    "        date_period_list.append(target_date.strftime(\"%Y-%m-%d\"))\n",
    "        target_date += delta\n",
    "\n",
    "    return date_period_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---------- BEGINNING FETCHING DATA ---------->\n",
      "Data for facility 12 saved!\n",
      "Data for facility 13 saved!\n",
      "<----- Skipping facility 14 ----->\n",
      "Data for facility 15 saved!\n",
      "Data for facility 16 saved!\n",
      "Data for facility 17 saved!\n",
      "Data for facility 18 saved!\n",
      "Data for facility 19 saved!\n",
      "Data for facility 20 saved!\n",
      "Data for facility 21 saved!\n",
      "Data for facility 22 saved!\n",
      "Data for facility 23 saved!\n",
      "Data for facility 24 saved!\n",
      "Data for facility 25 saved!\n",
      "Data for facility 26 saved!\n",
      "Data for facility 27 saved!\n",
      "Data for facility 28 saved!\n",
      "Data for facility 29 saved!\n",
      "Data for facility 30 saved!\n",
      "Data for facility 31 saved!\n",
      "Data for facility 32 saved!\n",
      "Data for facility 33 saved!\n",
      "<---------- COMPLETED FETCHING DATA ---------->\n"
     ]
    }
   ],
   "source": [
    "print(\"<---------- BEGINNING FETCHING DATA ---------->\")\n",
    "\n",
    "for facility_id in range(12,34):\n",
    "  # restrict duplicate data\n",
    "  if facility_id == 11 or facility_id == 14:\n",
    "    print(f\"<----- Skipping facility {facility_id} ----->\")\n",
    "    continue\n",
    "  \n",
    "  \n",
    "  get_carpark_history(facility_id, date_getter(timedelta(days=91)))\n",
    "  \n",
    "print(\"<---------- COMPLETED FETCHING DATA ---------->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging existing data from 1st Oct'23 to 31st Dec'23(path @ data/carpark_history_3_months) to new data from 1st Jul'23 to 30th Sept'23 (path @ data/carpark_history_prev_3_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with facility 6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m first_3_months_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/carpark_history_prev_3_months/facility_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfacility_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m last_3_months_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/carpark_history_3_months/facility_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfacility_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 15\u001b[0m first_3_months_file \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(first_3_months_file_path,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m last_3_months_file \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(last_3_months_file_path)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Adding the facility data into one dataframe\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\mwiti-base\\Lib\\site-packages\\pandas\\io\\json\\_json.py:804\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\mwiti-base\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1014\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[0;32m   1017\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[0;32m   1018\u001b[0m     )\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\mwiti-base\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1040\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1038\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1040\u001b[0m     obj \u001b[38;5;241m=\u001b[39m FrameParser(json, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\mwiti-base\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1173\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse()\n\u001b[0;32m   1175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\mwiti-base\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1366\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1362\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1366\u001b[0m         ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1369\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1370\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1372\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for facility_id in range(6,34):\n",
    "  print(f'Dealing with facility {facility_id}')\n",
    "  \n",
    "  if facility_id == 11 or facility_id == 14:\n",
    "    file = pd.read_json(f'data/carpark_history_6_months/facility_{facility_id}.json')\n",
    "    \n",
    "    df = pd.concat([df, file])\n",
    "    print(f'Done with facility {facility_id}')\n",
    "    continue\n",
    "  first_3_months_file_path = f'data/carpark_history_prev_3_months/facility_{facility_id}.json'\n",
    "  last_3_months_file_path = f'data/carpark_history_3_months/facility_{facility_id}.json'\n",
    "  \n",
    "  first_3_months_file = pd.read_json(first_3_months_file_path,encoding='latin-1')\n",
    "  last_3_months_file = pd.read_json(last_3_months_file_path)\n",
    "  \n",
    "  # Adding the facility data into one dataframe\n",
    "  facility_6_months_data = pd.concat([first_3_months_file, last_3_months_file])\n",
    "  \n",
    "  # Combining facility data with existing dataframe with all the data\n",
    "  df = pd.concat([df, facility_6_months_data])\n",
    "  \n",
    "  print(f'Done with facility {facility_id}')\n",
    "\n",
    "# Saving the file as a parquet file\n",
    "df.to_parquet('data/carpark_history_6_months.parquet')\n",
    "\n",
    "# Feedback\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVIOUS WORK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('data/weather/temperature.csv')\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_getter(combined_date):\n",
    "  date = combined_date.split(' ')[0]\n",
    "  time = combined_date.split(' ')[1]\n",
    "  return date, time\n",
    "\n",
    "df_temp[['actual_date', 'actual_time']] = df_temp['date'].apply(lambda x: pd.Series(date_time_getter(x)))\n",
    "# df_temp['actual_date'] = pd.to_datetime(df_temp['actual_date'])\n",
    "\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_temp.copy()\n",
    "\n",
    "df.drop(columns='date',inplace=True)\n",
    "df = df[['actual_date','actual_time','temperature']] \n",
    "df.rename(columns = {'actual_date':'date','actual_time':'time'}, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_temperatures_per_hour(df):\n",
    "    # Convert 'time' column to datetime object\n",
    "    df['time'] = pd.to_datetime(df['time'],format=\"%H:%M:%S\")\n",
    "    \n",
    "    # Extract hour component from 'time' column\n",
    "    df['hour'] = df['time'].dt.hour\n",
    "    \n",
    "    # Group by 'date' and 'hour' and calculate mean temperature\n",
    "    aggregated_df = df.groupby(['date', 'hour'])['temperature'].mean().reset_index()\n",
    "    \n",
    "    return aggregated_df\n",
    "  \n",
    "df_aggregate = aggregate_temperatures_per_hour(df)\n",
    "df_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = pd.read_json('data/carparks_original.json')\n",
    "cp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coords_df = pd.read_json('data/coords.json')\n",
    "coords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = cp.copy()\n",
    "\n",
    "final_df = final_df.merge(coords_df,on='facility_id',how='left')\n",
    "\n",
    "final_df.sort_values(by='facility_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_parquet('data/carpark_history_3_months_combined.parquet')\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
